---
title: "Land Bank AI"
author: "Lucia Walinchus"
date: "Summer 2021"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(lubridate)
library(DT)
library(ggmap)
library(XML)
library(purrr)
library(leaflet)
library(sf)
library(readxl)
library(tidymodels)
library(themis)

```



We got this from a FOIA from the Cuyahoga County Fiscal Office

```{r cars}
TransferHistory <- rio::import("TRANSFER_HISTORY_CUYAHOGA_COUNTY.txt") # Warning, after you start this, you should probably go to lunch. This takes a ridiculously long time to load. 
```

Oh boy ~2.8 million transfers! 

```{r pressure, echo=FALSE}
#Date is a Character, and we need the recorded date as a date
TransferHistory$Transfer_date <- dmy(TransferHistory$TRANSFER_DATE)

```




County Land Banks (not to be confused with city land banks) have only been around though since 2009, and so we will only need that. 

For reference- SB 353 analysis by the Legislative Service Commission: https://www.lsc.ohio.gov/documents/gaDocuments/analyses127/08-sb353-127.pdf

_"Authorizes  a  county  with  a  population  exceeding  1.2  million  to  form,  within  one  year  of  the  act's  effective  date,  a  county  land  reutilization  corporation   (CLRC),   a   nonprofit   corporation,   for   the   purposes   of   promoting  development  and  managing  and  facilitating  the  reclamation,  rehabilitation,  and  reutilization  of  vacant,  abandoned,  tax-foreclosed,  or  other real property."_
And the effective date is April 7, 2009.

[Side note, really weird, the date function reads "68" as 2068" not "1968." For our purposes, we are only going to look at county land banks, as city land banks don't really get the party started.And that doesn't happen until 2009. But that's why there are several errors that come up as dated in the future.]


```{r}
Transfer_History_Post_April09 <-filter(TransferHistory, Transfer_date>"2009-04-06")
rm(TransferHistory)# To give your computer memory a break
```



Through  additional FOIAs:

Note: we removed 16 listings of bad data. Which isn't bad for 48,852 transactions in 2020, but still not ideal.

```{r}
Cuyahoga2020 <- rio::import("ENTIRE COUNTY_SalesListing_2020_EntireYear.xlsx")


setwd("~/Code/Blue/Housing_Equity/Cuyahoga 2021 Transfers/")

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.txt$", 
                            full.names = TRUE)


Cuyahoga2021 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_csv, progress = show_progress())

```

Notes: 006-02-013 in 2020 had an error causing an extra column. This property is never taken over by the land bank so excluded for our purposes




Putting them together and in the same format: 

```{r}
Cuyahoga2020_2021 <- rbind(Cuyahoga2020,Cuyahoga2021)

Cuyahoga2020_2021 <- Cuyahoga2020_2021 %>% 
  rename(PROPERTY_NUMBER=Parcel, 
         Transfer_date=`Sale Date`,
         GRANTOR1=Grantor,
         GRANTEE1=Grantee,
         DEED_TYPE=`Deed Type`,
         AUTO_FILE_NUMBER=AFN) %>% 
  select(PROPERTY_NUMBER, 
         Transfer_date,
         GRANTOR1,
         GRANTEE1,
         DEED_TYPE,
         AUTO_FILE_NUMBER)


Transfer_History_Post_April09 <- Transfer_History_Post_April09 %>% 
  select(PROPERTY_NUMBER, 
         Transfer_date,
         GRANTOR1,
         GRANTEE1,
         DEED_TYPE,
         AUTO_FILE_NUMBER) %>% 
  rbind(Cuyahoga2020_2021)

```






How many properties are going to and from land banks? Or were forfeited to the state?

Going TO landbanks
```{r}
Land_Bank_Transfers <- Transfer_History_Post_April09 %>% 
  filter(str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF"))
summary(Land_Bank_Transfers)
```
From Land banks to others. (Note City and County have many between them.)
```{r}
Land_Bank_Recipients <- Transfer_History_Post_April09 %>% 
  filter(str_detect(GRANTOR1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF"))
summary(Land_Bank_Recipients)
```


How did Covid affect transfers? 
```{r}
Land_Bank_Transfers %>% 
  count(year(Transfer_date))
```

Fewer, also affected by litigation. 

```{r}
Land_Bank_Recipients %>% 
  count(year(Transfer_date))
```




Who got the most land bank transfers? 

```{r}
Largest_Land_Bank_Recipients <- Land_Bank_Recipients %>% 
  group_by(GRANTEE1) %>% 
  summarize(Total=n())

datatable(Largest_Land_Bank_Recipients)


```





How many properties went from one to another? 

Note: we can't just look for stuff with "Land" in it since "land" is in "Cleveland."


```{r}
To_and_from_Land_Banks <- rbind(Land_Bank_Recipients, Land_Bank_Transfers) %>% 
  filter(str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization") & str_detect(GRANTOR1, "land bank|LAND BANK|REUTILIZATION|Reutilization")) %>% 
  distinct()
  


```


These are the properties that went from one land bank to another. This is pretty common as county land banks came later and had more control over things. 

```{r}
Properties_That_Switched_Land_Banks <- Transfer_History_Post_April09 %>% 
  filter(PROPERTY_NUMBER %in% To_and_from_Land_Banks$PROPERTY_NUMBER)

```


Where exactly are these properties? First, let's bring in our parcel listing. 


Getting unique land banks properties
```{r}

Land_Bank_Dups <- Land_Bank_Transfers %>% 
  group_by(PROPERTY_NUMBER) %>% 
  summarize(Total=n()) 
  

datatable(Land_Bank_Dups)

```

Some parcels went to the land bank multiple times. Or were transferred between land banks. So for our purposes, we want to make sure they only come up once. 

```{r}
#Land_Bank_Unique  <- Land_Bank_Transfers %>% 
 #distinct(PROPERTY_NUMBER, .keep_all = TRUE)
```

Now where are all these properties? 
To locate them, we will need the list of all parcels, to match the IDs: 

```{r}
Cuyahoga_Properties <- rio::import("CUYAHOGA_PARCELLISTING_2019.csv") #also from a FOIA
```

And we're going to merge them by the property number. Note the parcel id and property number are the same except the property number has dashes. 

```{r}
Cuyahoga_data <- left_join(Land_Bank_Transfers, Cuyahoga_Properties, by = "PROPERTY_NUMBER")
```


Here's a challenge: the *property* address isn't always the *mailing* address. 

Second challenge: Some of these properties were demolished such as parcel number 015-06-060 which went through the land bank, was demolished, and now that property doesn't exist. So there is no property number. This is actually exactly what land banks are supposed to do! But this makes our job a lot harder. 


```{r}
#Cuyahoga_demolished <- Cuyahoga_data %>% 
  #filter(is.na(PROPERTY_NUMBER.x))

```
Not in the code: We found each address by hand. 
![This involved going through hundreds of old property records, maps, and approximating where these properties were located.]("example of going through property photos- Cuyahoga.png")

In addition, there are approximately with 1,937 parcels with a property number that still exists but with no address. These are empty lots. 

To map this data, we used parcel shapefiles from [Cuyahoga County's Open Data Portal.](https://data-cuyahoga.opendata.arcgis.com/) Specifically, we want to open and combine the shapefiles for Cleveland and non-Cleveland so we can map this and just get an idea of where these properties are. 

```{r}
setwd("~/Code/Blue/Housing_Equity/Combined_Parcels_-_Non-Cleveland_Only")
Non_Cleveland_Geo <- sf::st_read("Combined_Parcels_-_Non-Cleveland_Only.shp")
head(Non_Cleveland_Geo)

setwd("~/Code/Blue/Housing_Equity/Combined_Parcels_-_Cleveland_Only")
Cleveland_Geo <- sf::st_read("Combined_Parcels_-_Cleveland_Only.shp")
head(Non_Cleveland_Geo)

Cuyahoga_geo <- rbind(Non_Cleveland_Geo, Cleveland_Geo)
```

This is awesome but for some reason the column names are not the same as the website, so fixing that. 

```{r}
Cuyahoga_geo <- Cuyahoga_geo %>% 
  rename(parcel_year=parcel_yea,
         Transfer_date=transfer_d,
         Tax_Legal_Use_Description=tax_luc_de,
         Property_Class=property_c,
         Certified_Tax_Legal_Use_Classification=certified_,
         Certified_Tax_Land=certifie_1,
         Certified_Tax_Building=certifie_2,
         Certified_Tax_Total=certifie_3,
         Certified_Exempt_LUC=certifie_4,
         Certified_Exempt_Land=certifie_5,
         Certified_Exempt_Building=certifie_6,
         Certified_Exempt_Total=certifie_7,
         Certified_Abated_LUC=certifie_8,
        Certified_Abated_Land=certifie_9,
          Certified_Abated_Building=certifie10,
          Certified_Abated_Total=certifie11,
        Gross_Certified_Land=gross_cert,
         Gross_Certified_Building=gross_ce_1,
         Gross_Certified_Total=gross_ce_2,
         Residential_building_count=res_bldg_c,
         Total_residential_Living_Area=total_res_,
         Total_residential_rooms=total_re_1,
         Commerical_Building_Count=com_bldg_c,
         Total_Commerical_UseArea=total_com_)
```





```{r}
Cuyahoga_data_with_map <- merge(Cuyahoga_data, Cuyahoga_geo, by.x="PARCEL_ID",by.y="parcel_id")
```

Exporting this to vizualize it
```{r vizualizing it}
Cuyahoga_data_with_map <- st_as_sf(Cuyahoga_data_with_map)

plot(Cuyahoga_data_with_map %>% select(PARCEL_ID, geometry))

```

Adding a background map so we can zoom in and out.
```{r}
Cuyahoga_Properties_plotted <- leaflet(data = Cuyahoga_data_with_map) %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons()
  
Cuyahoga_Properties_plotted
```


Now we need to bring in the parcel numbers that no longer exist. (This was done manually, by going through the chain of title.)

So this is two different types of map data: parcels and points. Since these parcels no longer exist, we can't get their polygons, so we had to find their addresses and then geocode them to get their points.  For visualization purposes, we will want to see where these are. And some addresses have more than one property number. (And some property numbers have more than one address, as in apartments.) 

So we also need the version with the property numbers for the machine learning part. 

Bringing in the demolished parcels with approximate addresses
```{r}
Cuyahoga_demolished_with_Addresses <- rio::import("Cuyhoga Properties demolished with approximate addresses.csv")
```

Bringing in the geocoded parcels

```{r}
Cuyahoga_demolished_with_Addresses_and_Geocoded <- rio::import("Cuyahoga demolished with addresses and geocoded.csv")

```

Adding the geocoded parcels to the map:
```{r}
Cuyahoga_Properties_plotted_all <- leaflet(data = Cuyahoga_data_with_map) %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons() %>% 
  addMarkers(lat = Cuyahoga_demolished_with_Addresses_and_Geocoded$lat, lng = Cuyahoga_demolished_with_Addresses_and_Geocoded$lon, popup = "address")
  
Cuyahoga_Properties_plotted_all
```

Some parcels were subsumed into other parcels that had never been taken over by the land bank, some became part of other land bank properties, etc. 


Wheare are political donors in relation to this map? Note: Some listed home and some listed their work adddress. 

```{r}
setwd("~/Code/Blue/Housing_Equity/Political Contributions")

Cuyahoga_Campaign_Contributions <- rio::import("County Campaign Contributions Cuyahoga_geocoded_v3.0.csv")
Cuyahoga_Campaign_Contributions$Amount <- as.double(Cuyahoga_Campaign_Contributions$Amount)

```

```{r}
Cuyahoga_Properties_plotted_with_Contributions <- leaflet() %>% 
  addTiles(data = Cuyahoga_data_with_map) %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(data = Cuyahoga_data_with_map) %>% 
  addMarkers(lat = Cuyahoga_demolished_with_Addresses_and_Geocoded$lat, lng = Cuyahoga_demolished_with_Addresses_and_Geocoded$lon, popup = Cuyahoga_demolished_with_Addresses_and_Geocoded$address) %>% 
  addCircleMarkers(lat = Cuyahoga_Campaign_Contributions$lat, lng =Cuyahoga_Campaign_Contributions$lon, color ="red", popup =Cuyahoga_Campaign_Contributions$address)
  
Cuyahoga_Properties_plotted_with_Contributions
```

Doesn't look like there's much overlap betwen political contributors of Budish and Gallagher and land bank properties. Though note: in Cuyahoga county, nine people actually adjudicate the BOR forclosures, not the BOR itself. We got that list of people through 



What properties were behind on their taxes an potentially subject to being taken over?


```{r}
Cuyahoga_Delinquent_2018 <- rio::import("DelnqntTaxPayers_Entire County JAN 23 2019.xlsx")#they have to be certified in January that they didn't pay all of 2018

setwd("~/Code/Blue/Housing_Equity/Cuyahoga_Delinquent_2019")

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.xlsx$", 
                            full.names = TRUE)

Cuyahoga_Delinquent_2019 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_excel, progress = show_progress())

setwd("~/Code/Blue/Housing_Equity/Cuyahoga_Delinquent_2020")

Cuyahoga_Delinquent_2019 <- Cuyahoga_Delinquent_2019 %>% filter(SCHOOL_DESCR!="Delinquent is defined as having a balance greater than $0.00 and no payment plan  after 2nd half collection closed.CUYAHOGA COUNTY ASSUMES NO LIABILITY FOR DAMAGES AS A RESULT OF ERRORS, OMISSIONS OR DISCREPANCIES CONTAINED IN THESE PAGES. PROSPECTIVE PURCHASERS SHOULD CONSULT A REAL ESTATE ATTORNEY AND PURCHASE A TITLE INSURANCE POLICY PRIOR TO THE SALE.") %>% filter(!is.na(SCHOOL_DESCR))

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.xlsx$", 
                            full.names = TRUE)

Cuyahoga_Delinquent_2020 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_excel, col_types ="text", progress = show_progress())


Cuyahoga_Delinquent_2020 <- Cuyahoga_Delinquent_2020 %>% filter(SCHOOL_DESCR!="Delinquent is defined as having a balance greater than $0.00 and no payment plan  after 2nd half collection closed.CUYAHOGA COUNTY ASSUMES NO LIABILITY FOR DAMAGES AS A RESULT OF ERRORS, OMISSIONS OR DISCREPANCIES CONTAINED IN THESE PAGES. PROSPECTIVE PURCHASERS SHOULD CONSULT A REAL ESTATE ATTORNEY AND PURCHASE A TITLE INSURANCE POLICY PRIOR TO THE SALE.") %>% filter(!is.na(SCHOOL_DESCR))
```




Adding those together

```{r}
Cuyahoga_Delinquent_2018 <- Cuyahoga_Delinquent_2018 %>% 
  rename(SCHOOL_DESCR=SCHOOL_DIST_NAME) %>% 
  select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquent_2019 <- Cuyahoga_Delinquent_2019 %>% select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquent_2020 <- Cuyahoga_Delinquent_2020 %>% select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquents <- rbind(Cuyahoga_Delinquent_2018, Cuyahoga_Delinquent_2019,Cuyahoga_Delinquent_2020)
head(Cuyahoga_Delinquents)

```



Some properties might be delinquent both years. 
```{r}
  Cuyahoga_Delinquents %>% count(PROPERTY_NUMBER)
```

So we need to keep only the distinct ones

```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>%  distinct(PROPERTY_NUMBER, .keep_all = TRUE)
```




That's a lot of properties. 

It appears that not all those properties are actually delinquent though. For example, 120-19-003 is a dorm at Case Western reserve. 

We asked them why they owed ~$4 million in taxes, they said they were exempt. 
!("Case Western taxes.jpeg")

Here is a reference to the [Legal Use Codes in Ohio.]("https://codes.ohio.gov/ohio-administrative-code/rule-5703-25-10")
So though their Classification ID is in the commercial taxable bracket, In the (400-0s) their exempt Land Use Code makes them exempt from tax. (In the 600-0s)

So we need to determine what the exempt legal use code is, and filter that out. 
note: properties in the 700-0s are exempt because they are in the land bank essentially 

```{r}
Cuyahoga_Delinquents$CLASSIFICATION_ID <- as.double(Cuyahoga_Delinquents$CLASSIFICATION_ID)

Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  filter(CLASSIFICATION_ID<6000|CLASSIFICATION_ID>6999|is.na(CLASSIFICATION_ID))
```




Which parcels were eventually foreclosed upon?
First, our main chunk of properties. Then, the property numbers that were demolished, which were all LB properties.

We want to make sure that we get the folks who were late on their taxes and THEN the government chose to foreclose on them. (So we don't tage anyone who got their property from the land bank and then was late on their taxes.) 

You have to not pay your taxes for a whole year in Ohio, then they are certified late by the county Auditor. So Tax Year 2018 means your delinquent certification was actually issued in January of 2019. 

So for our purposes, we want to look for property transfers to the land bank which took place after January 23, 2019. 


```{r}
Cuyahoga_data_with_map_post_Jan_2019 <- filter(Cuyahoga_data_with_map, Transfer_date>"2019-01-23")

Cuyahoga_demolished_with_Addresses$Transfer_date <- lubridate::dmy(Cuyahoga_demolished_with_Addresses$TRANSFER_DATE)
Cuyahoga_demolished_with_Addresses_post_Jan_2019 <- filter(Cuyahoga_demolished_with_Addresses, Transfer_date>"2019-01-23")

Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(Land_Bank=if_else(PROPERTY_NUMBER %in% Cuyahoga_data_with_map_post_Jan_2019$PROPERTY_NUMBER|PROPERTY_NUMBER %in% Cuyahoga_demolished_with_Addresses_post_Jan_2019$PROPERTY_NUMBER,1,0))

```

 

How many is that? What percent of 2018-2019-2020 delinquent properties were taken over by the government or land bank in 2019, 2020, or 2021?

```{r}
sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)
nrow(Cuyahoga_Delinquents)

sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)/(sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)+
nrow(Cuyahoga_Delinquents))#Order of operations
```


And putting the amount owed in numeric form: 

```{r}
Cuyahoga_Delinquents$GRAND_TOTAL_OWED <- as.double(Cuyahoga_Delinquents$GRAND_TOTAL_OWED)
Cuyahoga_Delinquents$GRAND_TOTAL_PAID <- as.double(Cuyahoga_Delinquents$GRAND_TOTAL_PAID)
Cuyahoga_Delinquents$GRAND_TOTAL_BALANCE <-as.double(Cuyahoga_Delinquents$GRAND_TOTAL_BALANCE)
```


The value of most land bank properties is hard to gauge. Some are worth essentially negative money, as they would be harder to fix up than to actually operate. 

Some of the people who are behind on their taxes aren't behind by much- for example, the 169 people who owe a cent- that's probably a typo. 


On the other had, a property in Montgomery county got foreclosed upon [for owing thirteen cents.](https://eyeonohio.com/wp-content/uploads/2020/02/Affidavit-of-Treasurer-for-parcel-number-13508151.pdf) They hadn't actually paid their tax bill in years, but they put that amount on the lien as the actual amount didn't matter- it's not like they would be collecting it anyway, they just needed to get a certified lien to foreclose upon. 

So to get a sense of how behind these properties are on their taxes, we will calculate the amount owed as a percentage of their certified total property value. 

```{r}
Cuyahoga_Delinquents$Certified_Total_Value <- Cuyahoga_Properties$CERT_TOT[match(Cuyahoga_Delinquents$PROPERTY_NUMBER, Cuyahoga_Properties$PROPERTY_NUMBER)]

Cuyahoga_Delinquents <-  Cuyahoga_Delinquents %>% 
  mutate(owed_as_percentage_of_lot_value=GRAND_TOTAL_BALANCE/Certified_Total_Value)
```

Note: 68 properties are listed as having a value of 0. This is on the website too, and there doesn't seem to be a clear pattern. We are adding this to our list to ask the fiscal officer. 

```{r}
Cuyahoga_Delinquents_with_0_value <- Cuyahoga_Delinquents %>% select(PROPERTY_NUMBER,Certified_Total_Value) %>% filter(Certified_Total_Value==0)

Cuyahoga_Delinquents <-  Cuyahoga_Delinquents %>% filter(Certified_Total_Value!=0)
```




So 2,362 properties went to the government, but that's only 2% of the properties. (And some ostensibly people paid their back taxes!) 

Next: we need to bring in the categories of each school district. 

This is from the Ohio Department of Education. [There's no direct download link, you have to choose what you want.](https://reports.education.ohio.gov/report/report-card-data-district-enrollment-by-student-demographic)

They don't allow you to download all the info at once by district, or if they do we couldn't figure it out. But anyway, this takes every school in Cuyahoga. (So we searched for "Cuyahoga," clicked on the button next to each one, then downloaded.) 


```{r}
Cuyahoga_schools_demographic <- rio::import("Cuyahoga Schools Info.xlsx")

head(Cuyahoga_schools_demographic)
```

A few things about this data: 
*They write "<10" for small categories less than 10, instead of an actual number. 
*Unlike on the website, this has decimals. How do you have part of a kid? Clarification from the Ohio Dept of education's Mandy Minick: "we count students as full time equivalents (FTEs) based on the length of time they are enrolled.  A student enrolled all year counts as 1.0 FTE in the total.  A student enrolled for half a year is 0.5 FTEs and so on.  We sum all the FTEs across the entire year (hence, the decimals). The spreadsheet on the left shows the decimals (33.188847), while the one on the right is rounded to the closest whole number for ease of reading."


How many kids are in each district?

```{r}
Cuyahoga_schools_demographic_no_remainders <- Cuyahoga_schools_demographic %>% 
  filter(CountOfEnrolledStudents!="<10")

  
Cuyahoga_schools_demographic_no_remainders$CountOfEnrolledStudents <- as.double(Cuyahoga_schools_demographic_no_remainders$CountOfEnrolledStudents)

Cuyahoga_school_pop <- Cuyahoga_schools_demographic_no_remainders%>% 
  group_by(District) %>% 
    summarize(Total=sum(CountOfEnrolledStudents))

datatable(Cuyahoga_school_pop)
```

Here we are going to treat race and socioeconomic status as factors, even though we have numbers. Why? 

These variables are going to affect our data in a categorical way. You either are discriminating on race or class or you are not. The amount of discrimination does not depend on the amount of people. For example, in the [Kaggle Titantic competition](https://www.kaggle.com/c/titanic) (which is great way to learn machine learning!) The passengers' ticket price is listed. Which passengers were more likely to survive?

You are more likely to survive _if you have a first class ticket_. Period. If your first class ticket costs twice as much as the next first class ticket, that didn't make you twice as likely to survive as someone with a ticket half that price. 

Which school districts have large minority populations? Which have populations with a high number of kids in poverty? 


Finding percentage of minority by District: 
```{r}
Cuyahoga_schools_by_race <- Cuyahoga_schools_demographic_no_remainders %>% 
  group_by(District,`Race/Ethnicty`,) %>% #typo in original for 'ethnicity'
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Cuyahoga_schools_by_race)
```

```{r}
Cuyahoga_schools_by_race %>% 
  filter(`Race/Ethnicty`=="White, Non-Hispanic") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```




Finding percent disadvantaged by district: 
 
```{r}
Cuyahoga_schools_by_poverty <- Cuyahoga_schools_demographic_no_remainders %>% 
  group_by(District,Subgroup,) %>% 
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Cuyahoga_schools_by_poverty)
```

```{r}
Cuyahoga_schools_by_poverty %>% 
  filter(Subgroup=="Economic Disadvantaged") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```


And categorizing them: 

```{r}
Cuyahoga_schools_by_race_with_majority_white <- Cuyahoga_schools_by_race %>% 
  group_by(District) %>% 
summarize(Majority_White=if_else(`Race/Ethnicty`=="White, Non-Hispanic"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_White_District=if_else(sum(Majority_White)==1,1,0))
  

Cuyahoga_schools_with_majority_disadvantaged <- Cuyahoga_schools_by_poverty %>% 
  group_by(District) %>% 
summarize(Majority_Disadvantaged=if_else(Subgroup=="Economic Disadvantaged"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_Disadvantaged_District=if_else(sum(Majority_Disadvantaged)==1,1,0))

  
```

How are the school districts represented in our delinquent dataset?

```{r}
Cuyahoga_Delinquents_by_School_District <- Cuyahoga_Delinquents2018_2019 %>% group_by(School_District) %>% summarize(Total=n())
datatable(Cuyahoga_Delinquents_by_School_District)

```



The names of school districts is the same but listed a bit differently in both data sets, so we have to do a bit of manual matching here. 

```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(School_Dest_Standardized= case_when(
     SCHOOL_DESCR=="CLEVELAND MSD" ~ "Cleveland Municipal - 043786 (Cuyahoga)",
     SCHOOL_DESCR=="CLEVELAND CSD" ~ "Cleveland Municipal - 043786 (Cuyahoga)", #Not sure why this is in here two ways but properties on the website listed as MSD here are CSD on Auditor's site
     SCHOOL_DESCR=="BEREA CSD" ~ "Berea City - 043612 (Cuyahoga)",                              
SCHOOL_DESCR=="SHAKER HEIGHTS CSD"  ~   "Shaker Heights City - 044750 (Cuyahoga)",                 
SCHOOL_DESCR=="BAY VILLAGE CSD" ~ "Bay Village City - 043547 (Cuyahoga)" ,                       
SCHOOL_DESCR=="WESTLAKE CSD"~   "Westlake City - 045062 (Cuyahoga)",                        
SCHOOL_DESCR=="NORTH OLMSTED CSD" ~ "North Olmsted City - 044529 (Cuyahoga)",                     
SCHOOL_DESCR=="OLMSTED FALLS CSD" ~"Olmsted Falls City - 046573 (Cuyahoga)",                     
SCHOOL_DESCR=="ROCKY RIVER CSD"~  "Rocky River City - 044701 (Cuyahoga)",                      
SCHOOL_DESCR=="LAKEWOOD CSD" ~  "Lakewood City - 044198 (Cuyahoga)",                        
SCHOOL_DESCR=="FAIRVIEW PARK CSD"  ~ "Fairview Park City - 043976 (Cuyahoga)",                     
SCHOOL_DESCR=="STRONGSVILLE CSD"~ "Strongsville City - 044842 (Cuyahoga)" ,                     
SCHOOL_DESCR=="BROOKLYN CSD" ~  "Brooklyn City - 043653 (Cuyahoga)",                         
SCHOOL_DESCR=="PARMA CSD"~ "Parma City - 044636 (Cuyahoga)",                              
SCHOOL_DESCR=="NORTH ROYALTON CSD"  ~  "North Royalton City - 044545 (Cuyahoga)",                  
SCHOOL_DESCR=="BRECKSVILLE-BROADVIEW HEIGHTS SD"  ~  "Brecksville-Broadview Heights City - 043646 (Cuyahoga)",  
SCHOOL_DESCR=="BRECKSVILLE-BROADVIEW HEIGHTS" ~  "Brecksville-Broadview Heights City - 043646 (Cuyahoga)", 
SCHOOL_DESCR=="CUYAHOGA HEIGHTS LSD"~  "Cuyahoga Heights Local - 046557 (Cuyahoga)",           
SCHOOL_DESCR=="GARFIELD HEIGHTS CSD" ~  "Garfield Heights City Schools - 044040 (Cuyahoga)" ,                
SCHOOL_DESCR=="INDEPENDENCE LSD" ~ "Independence Local - 046565 (Cuyahoga)",                       
SCHOOL_DESCR=="EUCLID CSD"~  "Euclid City - 043950 (Cuyahoga)",                            
SCHOOL_DESCR=="RICHMOND HEIGHTS LSD" ~   "Richmond Heights Local - 046599 (Cuyahoga)",               
SCHOOL_DESCR=="SOUTH EUCLID-LYNDHURST CSD" ~ "South Euclid-Lyndhurst City - 044792 (Cuyahoga)",            
SCHOOL_DESCR=="EAST CLEVELAND CSD"~ "East Cleveland City School District - 043901 (Cuyahoga)",                     
SCHOOL_DESCR=="CLEVELAND HEIGHTS-UNIVERSITY HEIGHTS SD"~"Cleveland Heights-University Heights City - 043794 (Cuyahoga)",
SCHOOL_DESCR=="CLEVELAND HTS-UNIVERSITY HTS C"~"Cleveland Heights-University Heights City - 043794 (Cuyahoga)", #also written two ways
SCHOOL_DESCR=="WESTLAKE CSD" ~ "Westlake City - 045062 (Cuyahoga)",
SCHOOL_DESCR=="BEACHWOOD CSD"~  "Beachwood City - 043554 (Cuyahoga)",                         
SCHOOL_DESCR=="WARRENSVILLE HEIGHTS CSD"~ "Warrensville Heights City - 045005 (Cuyahoga)",               
SCHOOL_DESCR=="ORANGE CSD" ~  "Orange City - 046581 (Cuyahoga)",                          
SCHOOL_DESCR=="MAPLE HEIGHTS CSD"~  "Maple Heights City - 044305 (Cuyahoga)",                     
SCHOOL_DESCR=="BEDFORD CSD"  ~  "Bedford City - 043562 (Cuyahoga)",                         
SCHOOL_DESCR=="MAYFIELD CSD" ~ "Mayfield City - 044370 (Cuyahoga)",                         
SCHOOL_DESCR=="CHAGRIN FALLS EVSD" ~"Chagrin Falls Exempted Village - 045286 (Cuyahoga)",                     
SCHOOL_DESCR=="SOLON CSD" ~ "Solon City - 046607 (Cuyahoga)" ))                            



```

And here we incorporate race and socioeconomic status as factors. 

```{r}
Cuyahoga_Delinquents$is_school_district_majority_white <- Cuyahoga_schools_by_race_with_majority_white$Majority_White_District[match(Cuyahoga_Delinquents$School_Dest_Standardized,Cuyahoga_schools_by_race_with_majority_white$District)]

Cuyahoga_Delinquents$is_school_district_majority_disadvantaged <- Cuyahoga_schools_with_majority_disadvantaged$Majority_Disadvantaged_District[match(Cuyahoga_Delinquents$School_Dest_Standardized,Cuyahoga_schools_with_majority_disadvantaged$District)]


```


Some exploratory analysis: 

Of the parcels taken over by the land bank, how many are in majority white districts? How does that compare the number of delinquent taxpayers who are in majority white districts?
```{r}
prop.table(table(Cuyahoga_Delinquents$is_school_district_majority_white, Cuyahoga_Delinquents$Land_Bank))
```
On the left, in the column, 0 represents the school district is not majority white, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.


What percentage of delinquent properties are in majority white school districts? What percentage of properties that go to the land bank are in majority white school districts?
```{r}

Cuyahoga_Delinquents %>% group_by(Land_Bank) %>% count(is_school_district_majority_white)
```




And majority disadvantaged? How does that compare to the number of delinquent taxpayers who are in majority disadvantaged districts?

```{r}
prop.table(table(Cuyahoga_Delinquents$is_school_district_majority_disadvantaged, Cuyahoga_Delinquents$Land_Bank))
```
On the left, in the column, 0 represents the school district is not majority disadvantaged, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.









Now we're trying to find: are officials more like to give parcels TO people if it's close to a big amenity? (Like a college or hospital?) To "spur development?"




###Machine Learning


Okay step one: choosing an appropriate model. We want to predict here whether a property will end up in the land bank or not. So we are looking to build a Classification model. 

So first, we have very few Land bank properties versus delinquent properties overall. And this, of course, is our whole story. Who has the power to decide where this will happen? But we will have to take this into account and try to split our training and test sets with an equal number of land bank properties. 

Next, we are going to split the dataset into training and testing models, evenly diving the sections between the Land Bank and non-land bank properties. 

```{r}
stack_overflow <- Cuyahoga_Delinquents %>%
    mutate(Land_Bank = factor(Land_Bank, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_DESCR, -School_Dest_Standardized)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Land_Bank)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Land_Bank)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Land_Bank, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Land_Bank, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Land_Bank, estimate = .pred_glm)
accuracy(results, truth = Land_Bank, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Land_Bank, estimate = .pred_glm)
ppv(results, truth = Land_Bank, estimate = .pred_tree)
```

Which variables are particularly important? 

```{r}
data <- Cuyahoga_Delinquents %>%
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_DESCR, -School_Dest_Standardized)

names(data)

#writing a function to automate writing the formula: 
n<-names(data)
form1 = as.formula(paste("Land_Bank ~", paste(n[!n %in% "Land_Bank"], collapse = " + ")))
```

Here we are going to fit a multiple linear regression model for the number of Land bank properties using the rest of the variables as explanatory variables (predictors):



```{r}
MLModel = lm(form1,data =data)

MLModel
```

Let's see how well our model did. 

```{r}
summary(MLModel)
```
This is is terrible R value. So these factors do not matter as much. 


Which variables are particularly important? 

```{r}
data <- Cuyahoga_Delinquents %>%
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_DESCR)

names(data)

#writing a function to automate writing the formula: 
n<-names(data)
form1 = as.formula(paste("Land_Bank ~", paste(n[!n %in% "Land_Bank"], collapse = " + ")))
```

Here we are going to fit a multiple linear regression model for the number of Land bank properties using the rest of the variables as explanatory variables (predictors):



```{r}
MLModel = lm(form1,data =data)

MLModel
```

Let's see how well our model did. 

```{r}
summary(MLModel)
```

Not that well. So it looks like these factors aren't that important. 

Let's see where properties are in relation to future developments. 

Our Land_Bank_Recipients dataframe above takes properties where the land bank is the _grantor_ who then gives the property to the grantee. Legally, city land banks and county land banks are very different. County land banks have more powers, to get rid of tax liens and all other liens. So we want to look specifically at properties that went from the government to another entity (person or company) not just another land bank to hold onto. 


```{r}
Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients %>% 
  filter(!str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF|LAND REUIT|REUT|CITY OF CLEVELAND")) 
summary(Land_Bank_Recipients_Without_interlandbank_transfers)
```


```{r}
Land_Bank_Recipients_By_Type <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  mutate(type=if_else((str_detect(GRANTEE1,",")&!str_detect(GRANTEE1,"LLC")),"person","group"))


ggplot(Land_Bank_Recipients_By_Type)+
  aes(x=year(Transfer_date),fill=type)+
  geom_bar()+
  facet_wrap(~type)

         
```





Where are these properties?

```{r}
#Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
#distinct(PROPERTY_NUMBER, .keep_all = TRUE)

Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))



Land_Bank_Recipients_Without_interlandbank_transfers <- merge(Land_Bank_Recipients_Without_interlandbank_transfers, Cuyahoga_geo, by="parcel_id")
```


color = lubridate::year(Land_Bank_Recipients_Without_interlandbank_transfers$Transfer_date)


Where are the properties that went from a land bank TO a person within the past year?
```{r}
Land_Bank_Recipients_Without_interlandbank_transfers <- st_as_sf(Land_Bank_Recipients_Without_interlandbank_transfers)

Cuyahoga_Land_Bank_Recipient_Properties_plotted2021 <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  filter(year(Transfer_date.x)==2021) %>% 
  leaflet() %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(popup = Land_Bank_Recipients_Without_interlandbank_transfers$parcel_id)
  
Cuyahoga_Land_Bank_Recipient_Properties_plotted2021
```


How does that compare to properties the Land Bank is holding onto?

```{r}

Land_Bank_Recipients_Without_interlandbank_transfers_20_and_21 <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  filter(year(Transfer_date.x)>2019)


Cuyahoga_Land_Bank_Recipient_Properties_versus_ones_they_held_onto <- Cuyahoga_geo %>% 
  filter(str_detect(deeded_own, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF")) %>% 
  leaflet() %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(popup = Cuyahoga_geo$parcel_id, color = "red") %>% 
  addPolygons(data= Land_Bank_Recipients_Without_interlandbank_transfers_20_and_21, color="green") #%>% 
  #addPolygons(data = filter(Cuyahoga_geo, Gross_Certified_Total>1000000), color = "blue")
  
Cuyahoga_Land_Bank_Recipient_Properties_versus_ones_they_held_onto
  
```


Where are the expensive properties in Cuyahoga county?

What are the values of Cuyahoga parcels?

```{r}
ggplot(Cuyahoga_geo)+
  aes(x=Gross_Certified_Total)+
  geom_histogram(binwidth = 10000000)+
  stat_bin(binwidth= 10000000, geom="text", aes(label=..count..), vjust = -1) + 
scale_x_continuous(breaks = seq(0 , 702599700, 10000000) , labels = comma)+
theme(axis.text.x = element_text(angle = 45, vjust = 1.2, hjust = 1.1))
  geom_text(data=Cuyahoga_geo, mapping=aes(x=Gross_Certified_Total),label=Gross_Certified_Total)

hist(Cuyahoga_geo$Gross_Certified_Total, xlim=range(1000000),labels = TRUE)
```




We are going to set the (rather arbitrary) cutoff of $10 million parcels. 

```{r}
Cuyahoga_geo_millions <- filter(Cuyahoga_geo, Gross_Certified_Total>10000000)
```

How close are gov owed parcels to prime real estate? 

Adding mapping data to Cuyahoga Delinquents
```{r}

Cuyahoga_Delinquents_Geo <- Cuyahoga_Delinquents %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))

Cuyahoga_Delinquents_Geo$geometry <- Cuyahoga_geo$geometry[match(Cuyahoga_Delinquents_Geo$parcel_id,Cuyahoga_geo$parcel_id)]

Cuyahoga_Delinquents_Geo <- Cuyahoga_Delinquents_Geo %>% 
filter(CLASSIFICATION_ID<6000|CLASSIFICATION_ID>6999|is.na(CLASSIFICATION_ID)) 

raster::crs(Cuyahoga_Delinquents_Geo) <- "EPSG:4326" 
shape_proj<-st_transform(Cuyahoga_Delinquents_Geo, CRS("+proj=gnom +lat_0=90 +lon_0=-50"))


```
Figuring out how far apart they are

```{r }
library(lwgeom)

sf::sf_use_s2(FALSE)

Cuyahoga_Distance_Matrix <- as.data.frame(st_distance(Cuyahoga_Delinquents_Geo$geometry,Cuyahoga_geo_millions$geometry)) #This takes quite a while just FYI


%>% 
  #Sets column names
  
Cuyahoga_Distance_Matrix1 <- Cuyahoga_Distance_Matrix %>% 
  `colnames<-`(Cuyahoga_geo_millions$parcel_id) %>% 
  
  #Adds a column containing names so that each row now also has a name
  cbind(name =Cuyahoga_Delinquents_Geo$PROPERTY_NUMBER ) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.

```

Problem: 
> Cuyahoga_Delinquents_Geo$geometry[[74]]
MULTIPOLYGON EMPTY
> Cuyahoga_Delinquents_Geo$geometry[[73]]
MULTIPOLYGON (((-81.75264 41.48498, -81.75272 41.48475, -81.75307 41.48481, -81.75325 41.48485, -81.75318 41.48507, -81.75264 41.48498)))
Looks like Cuyahoga_Delinquents_Geo$geometry
Geometry set for 96236 features  (with 7024 geometries empty)
It appears that some parcels such as 001-15-805C have no polygon because they are condos. 
And therefore st_distance(Cuyahoga_Delinquents_Geo$geometry[[74]],Cuyahoga_geo_millions$geometry[[1]]) yields NA. 

However, that only means 4,685,008 of 601,285,648 distances couldn't be computed, or 0.007791651, less than 1%. So we are going to exclude them from our calculations, but wanted to note that. 

```{r}
Cuyahoga_Distance_Matrix <- Cuyahoga_Distance_Matrix %>% 
  filter(!is.na(V1))

Cuyahoga_Distance_Matrix1 <- Cuyahoga_Distance_Matrix1 %>% 
  filter(!is.na(`12123003`))

```


Also, this means that 7024 properties are  not available for analysis, which is about 7% of the properties behind on their taxes. But none of these properties ended up in the Land Bank. 

```{r}
Empty_Delinquents <- Cuyahoga_Delinquents_Geo %>% filter(st_is_empty(geometry))

sum(Empty_Delinquents$Land_Bank)
```

Looks like none of these properties end up the the Cuyahgoa GIS database because they are associated with other parcels. 






Okay now let's do some math to add distance to our machine learning set. 


First, let's add the parcel number
```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))
```

And change the 


Then writing a function to find the sum, in meters, of the property to all other valuable properties.  

```{r}
Cuyahoga_Distance_Matrix_Totals <- Cuyahoga_Distance_Matrix %>% 
 janitor::adorn_totals(where= "row", na.rm = TRUE, name="Total_Distance")

```


Doing this the other way, as this is causing a lot of issues to transpose it: 

```{r}
library(lwgeom)

sf::sf_use_s2(FALSE)

Cuyahoga_Distance_Matrix <- as.data.frame(st_distance(Cuyahoga_geo_millions$geometry,Cuyahoga_Delinquents_Geo$geometry)) #This takes quite a while just FYI, like several days
```

Making a similar matrix with the column names added, so we don't get confused: 

```{r}
#Sets column names
  
Cuyahoga_Distance_Matrix1 <- Cuyahoga_Distance_Matrix %>% 
  `colnames<-`(Cuyahoga_Delinquents_Geo$PROPERTY_NUMBER) %>% 
  
  #Adds a column containing names so that each row now also has a name
  #cbind(name = Cuyahoga_geo_millions$parcel_id) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.
```



Let's find the total distance away: 

```{r}
Cuyahoga_Distance_Matrix_Totals <- Cuyahoga_Distance_Matrix1 %>% 
  select(-name) %>% 
 janitor::adorn_totals(where= "col", na.rm = TRUE)


```

```{r}

Cuyahoga_Distance_Matrix_Totals <- Cuyahoga_Distance_Matrix_Totals %>% 
  slice(96237)
```

Dropping the meters units. Very cool that this is in meters but as we don't need to convert anything it's just making things more complicated.
```{r}
library(units)
Cuyahoga_Distance_Matrix_No_Units <- drop_units(Cuyahoga_Distance_Matrix1)

```
Finding the distance to all high-value parcels in the county. You can also do this with janitor's adorn_totals but we want to do a few Similar things so this is easier. 

```{r}
Total_Distance_To_High_Value_Parcels <- Cuyahoga_Distance_Matrix_No_Units %>%                                                       
  dplyr::summarise_all(~sum(., na.rm = TRUE))
```



Finding the number of high-value parcels within 500 meters of the delinquent properties. (Approximately the distance between Al Jenkins' property and the hospital.) 

```{r}
Number_Near <- Cuyahoga_Distance_Matrix_No_Units %>%                                                                                  
  dplyr::summarise_all(~sum(.<500, na.rm = TRUE))
```

And for comparison, also the number twice as far. 

```{r}
Number_Semi_Near <- Cuyahoga_Distance_Matrix_No_Units %>%                                                                           
  dplyr::summarise_all(~sum(.<1000, na.rm = TRUE))
```

```{r}
Cuyahoga_Distance_Info1 <-Number_Near %>% 
  select(-name) %>% 
  pivot_longer("001-02-033":"215-17-001", names_to = "PROPERTY_NUMBER", values_to = "number_of_high_value_properties_within_500m" )
```

```{r}
Cuyahoga_Distance_Info2 <-Number_Semi_Near %>% 
  select(-name) %>% 
  pivot_longer("001-02-033":"215-17-001", names_to = "PROPERTY_NUMBER", values_to = "number_of_high_value_properties_within_1000m" )
```

```{r}
Cuyahoga_Distance_Matrix_Totals <- drop_units(Cuyahoga_Distance_Matrix_Totals)

Cuyahoga_Distance_Info3 <-Cuyahoga_Distance_Matrix_Totals %>% 
  select(-Total) %>% 
  slice_tail(n=1) %>% 
  pivot_longer("001-02-033":"215-17-001", names_to = "PROPERTY_NUMBER", values_to = "Sum_of_distance_to_high_value_properties" )
```


Adding them together

```{r}
Cuyahoga_Delinquents_With_Distance_Info <- left_join(Cuyahoga_Delinquents, Cuyahoga_Distance_Info1,  by="PROPERTY_NUMBER")
Cuyahoga_Delinquents_With_Distance_Info <- left_join(Cuyahoga_Delinquents_With_Distance_Info, Cuyahoga_Distance_Info2,  by="PROPERTY_NUMBER")
Cuyahoga_Delinquents_With_Distance_Info <- left_join(Cuyahoga_Delinquents_With_Distance_Info, Cuyahoga_Distance_Info3,  by="PROPERTY_NUMBER")
```


###Machine Learning take two: with distance info

First, classification description didn't seem to be a good factor. Let's look into those: 
Which kids of properties are getting foreclosed  upon?

```{r}
Cuyahoga_Delinquents_Kinds <- Cuyahoga_Delinquents_With_Distance_Info  %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  summarize(Total_Gov__Foreclosure=sum(Land_Bank), Total_Behind=n()) %>% 
  ungroup() %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  mutate(Percent=Total_Gov__Foreclosure/sum(Total_Behind))
            
datatable(Cuyahoga_Delinquents_Kinds)
  
```
Two interesting things here. First, it appears there are 3 Land Bank Descriptions in the use code: 

```{r}
unique(Cuyahoga_Delinquents_With_Distance_Info$CLASSIFICATION_DESCR)
```
"City  (LAND BANKS)", "COUNTY LAND BANK" and "LAND REUTILIZATION (LAND BANKS)". So we have been undercounting some land bank properties when we got rid of duplicates. 

Fixing that. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info <- Cuyahoga_Delinquents_With_Distance_Info %>% 
  mutate(Gov_Foreclosure=ifelse(CLASSIFICATION_DESCR=="City  (LAND BANKS)"|CLASSIFICATION_ID=="COUNTY LAND BANK"|CLASSIFICATION_ID=="LAND REUTILIZATION (LAND BANKS)",1,Land_Bank))
```

Next, it seems no condos or house trailers are getting flagged here, even though they are the fourth and ninth highest in number.  
```{r}

Cuyahoga_Delinquents_Kinds <- Cuyahoga_Delinquents_With_Distance_Info  %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  summarize(Total_Gov__Foreclosure=sum(Gov_Foreclosure), Total_Behind=n()) %>% 
  ungroup() %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  mutate(Percent=Total_Gov__Foreclosure/sum(Total_Behind))
            
datatable(Cuyahoga_Delinquents_Kinds)
  
```

How much do they owe relative to other properties? 
```{r}
House_Trailers_Owed <- Cuyahoga_Delinquents_With_Distance_Info %>% filter(CLASSIFICATION_DESCR=="House trailer")
Single_Fam_Dwelling_Owed <- Cuyahoga_Delinquents_With_Distance_Info %>% filter(CLASSIFICATION_DESCR=="SINGLE FAMILY DWELLING")

summary(House_Trailers_Owed$owed_as_percentage_of_lot_value)
summary(Single_Fam_Dwelling_Owed$owed_as_percentage_of_lot_value)

```

Next, we are going to take out the properties that are no longer in the map database. It looks like these properties are now listed with other properties. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo <- filter(Cuyahoga_Delinquents_With_Distance_Info, !is.na(Sum_of_distance_to_high_value_properties))
```

What's the rough distribution?

```{r}
ggplot(Cuyahoga_Delinquents_With_Distance_Info_Geo, aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 100000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```
Looks like most owe less than $100,000 to start. Let's look closer at the distribution of that. 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED<100000) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 5000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```


What is the distribution of Land Bank properties?

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 10000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```
Looks like only a few over $210,000. 



    672-25-001
    EAST CLEVELAND LAND REUTILIZATION PROGRAM
    14600 EUCLID AVE
    EAST CLEVELAND, OH. 44112

An apartment building that was taken over. 



    126-21-014
    THE ZONE, LLC.
    2800 E 90 AVE
    CLEVELAND, OH. 44104


Yup looks like a warehouse that didn't pay taxes for years.Near a lot of construction on 90th.


Primary Owner
CITY OF CLEVELAND HEOIGHTS LAND REUTILIZATION PROGRAM
Property Address
1912 So Taylor RD Cleveland Hts,OH 44118
Tax Mailing Address
CITY OF CLEVELAND HEOIGHTS LAND REUTILIZATION 1900 S TAYLOR RD CLEVELAND, OH 44121
Legal Description
16 MONROE &S/L2 EP & 3 EP BUT TRI 0001 EP
Property Class
GENERAL RETAIL WITH WALK-UP APARTMENTS
Parcel Number
684-26-012
Taxset  
Cleveland Hts.

Yup apartments that didn't pay taxes for years. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  filter(GRAND_TOTAL_OWED<100000) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 10000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```

Of all foreclosures, how much was owed?



```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
what about the percent owed?

```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```


Okay step one: choosing an appropriate model. We want to predict here whether a property will end up in the land bank or not. So we are looking to build a Classification model. 

So first, we have very few Land bank properties versus delinquent properties overall. And this, of course, is our whole story. Who has the power to decide where this will happen? But we will have to take this into account and try to split our training and test sets with an equal number of land bank properties. 

Next, we are going to split the dataset into training and testing models, evenly diving the sections between the Land Bank and non-land bank properties. 

```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
  #filter(Certified_Total_Value>21900&Certified_Total_Value<105100) %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  select(Gov_Foreclosure, owed_as_percentage_of_lot_value) 
    #select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, -Sum_of_distance_to_high_value_properties,  -GRAND_TOTAL_OWED, -GRAND_TOTAL_PAID,  -Certified_Total_Value,  -Close_to_high_value_properties, -number_of_high_value_properties_within_500m, owed_as_percentage_of_lot_value
           

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```



Of the parcels taken over by the land bank, how many are in majority white districts? How does that compare the number of delinquent taxpayers who are in majority white districts?
```{r}
prop.table(table(Cuyahoga_Delinquents_With_Distance_Info_Geo$is_school_district_majority_white, Cuyahoga_Delinquents_With_Distance_Info_Geo$Gov_Foreclosure))
```
On the left, in the column, 0 represents the school district is not majority white, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.

So 75% of properties were not foreclosed upon and were in minority districts. About 22% of properties were in white district and were not foreclosed upon. Less than a thousandth of all properties were both in a white school district and were foreclosed upon. Three percent of all properties were foreclosed upon and were in a minority district. 


What percentage of delinquent properties are in majority white school districts? What percentage of properties that go to the land bank are in majority white school districts?
```{r}

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% group_by(Gov_Foreclosure) %>% count(is_school_district_majority_white)
```

In majority white school districts, there were 19,515 properties (19443+72). 0.00368947, or 72 out of the 19515 went to white districts. In majority black districts, there were 69,967 properties.(66906+2791). 2,791 of those went to the land bank, or 4 percent of those properties. 




How many expensive parcels within 500 m does the average/median tax delinquent property have? How many do Land Bank properties have?

Since the greatest factor is the grand total owed, we are going to take the amount of tax delinquent into account. 


What's the median owed as a percentage of the lot value? (Since we can't get how many years someone owed money on a property, this is a good proxy for the amount of time a property stood vacant.


```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```
What's the standard deviation?

```{r}
sd(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```
Let's look at properies around the mean. This is all properties: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

Then properties that weren't foreclosed on

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

And properties that were: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>%  
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
What's the standard deviation?

```{r}
sd(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
Let's look at properies around the mean. This is all properties: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>% 
  summarize(mean(number_of_high_value_properties_within_500m, na.rm=TRUE))
```

Then properties that weren't foreclosed on

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

And properties that were: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>%   
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```




```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  summarize(mean(is_school_district_majority_white))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(is_school_district_majority_white))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(is_school_district_majority_white))
```




```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  summarize(mean(is_school_district_majority_disadvantaged))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(is_school_district_majority_disadvantaged))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(is_school_district_majority_disadvantaged))
```

What happens when we take out the distances to high-value properties?



```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  #select(Gov_Foreclosure, owed_as_percentage_of_lot_value, number_of_high_value_properties_within_500m, number_of_high_value_properties_within_1000m, Sum_of_distance_to_high_value_properties) 
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, -Sum_of_distance_to_high_value_properties, -GRAND_TOTAL_BALANCE, -GRAND_TOTAL_OWED, -GRAND_TOTAL_PAID, -GRAND_TOTAL_BALANCE, -number_of_high_value_properties_within_1000m, -Certified_Total_Value, -is_school_district_majority_disadvantaged, -is_school_district_majority_white, -number_of_high_value_properties_within_500m, -number_of_high_value_properties_within_1000m
           )

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```




```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  #select(Gov_Foreclosure, owed_as_percentage_of_lot_value, number_of_high_value_properties_within_500m, number_of_high_value_properties_within_1000m, Sum_of_distance_to_high_value_properties) 
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, -Sum_of_distance_to_high_value_properties, -GRAND_TOTAL_BALANCE, -GRAND_TOTAL_PAID, -GRAND_TOTAL_BALANCE,  -Certified_Total_Value, -is_school_district_majority_disadvantaged, -is_school_district_majority_white, -owed_as_percentage_of_lot_value, -number_of_high_value_properties_within_500m
           )

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```








Let's trace the actual values against the predicted ones. 

##Allen County


To map this data, we used parcel shapefiles from [Allen County's Open Data Portal.](http://gis.allencountyohio.com/GIS/downloads.html) 

```{r}
setwd("~/Code/Blue/Housing_Equity/Allen County Current_Parcels/")

Allen_Geo <- sf::st_read("Current_Parcels.shp")
head(Allen_Geo)


```

Now we are going to bring in the Land Bank Parcels for the past few years. 

Through a FOIA: 

```{r}
Allen_County_LB_Properties <- rio::import("Allen County Land Bank Properties 1-1-16 to 7-1-21.xlsx")
```


Now bringing in Allen County's delinquent list. 

```{r}
Allen_Delinquent <- rio::import("Allen County 2021 03-12 total dlq taxpayer.xls")
```

To ask
Why are there two columns named prior delinquent real estate tax? 
What is the unnamed column?

This is amazing, though, there are tax delinquent properties that go way back, such as [this property labeled as delinquent in 1982.](http://allencountyohpropertytax.com/Tax.aspx?mpropertynumber=47-0610-03-001.000&p=47061003001.000)

What is the oldest delinquency?

```{r}
min(Allen_Delinquent$`CertDlq Year`)
```


```{r}
Allen_Delinquent <- Allen_Delinquent %>% 
  mutate(PARCEL=Parcel)
                             
Allen_County_LB_Properties <- Allen_County_LB_Properties %>% 
mutate(PARCEL=` Parcel Number`)


Allen_County_Delinquent_Geo <- Allen_Geo %>% 
  filter(PARCEL %in% Allen_County_LB_Properties$PARCEL|PARCEL %in% Allen_Delinquent $PARCEL)
```


Where are the high-value properties?

```{r}
Allen_Geo_Millions <- Allen_Geo %>% filter(MKTTOTVAL>10000000)
```


How close are they to our properties?

```{r }
library(lwgeom)

sf::sf_use_s2(FALSE)

Allen_Distance_Matrix <- as.data.frame(st_distance(  Allen_Geo_Millions$geometry, Allen_County_Delinquent_Geo$geometry,)) #This takes quite a while just FYI



  #Sets column names
  
Allen_Distance_Matrix1 <- Allen_Distance_Matrix %>% 
  `colnames<-`(Allen_County_Delinquent_Geo$PARCEL ) %>% 
  
  #Adds a column containing names so that each row now also has a name
  cbind(name = Allen_Geo_Millions$PARCEL  ) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.

```



Removing the units (which interestingly here are in feet.) 

```{r}
library(units)
Allen_Distance_Matrix_No_Units <- drop_units(Allen_Distance_Matrix1)
```


Finding the number of high-value parcels within half a mile of the delinquent properties. 

```{r}
Allen_Number_Near <- Allen_Distance_Matrix_No_Units %>%                                                                                  
  dplyr::summarise_all(~sum(.<2640, na.rm = TRUE))
```

And for comparison, also the number twice as far. 

```{r}
Allen_Number_Semi_Near <- Allen_Distance_Matrix_No_Units %>%                                                                           
  dplyr::summarise_all(~sum(.<5280, na.rm = TRUE))
```

```{r}
Allen_Distance_Info1 <-Allen_Number_Near %>% 
  select(-name) %>% 
  pivot_longer("18-3300-04-004.000":"48-3600-01-001.000", names_to = "PARCEL", values_to = "number_of_high_value_properties_within_half_mile" )
```

```{r}
Allen_Distance_Info2 <-Allen_Number_Semi_Near %>% 
  select(-name) %>% 
  pivot_longer("18-3300-04-004.000":"48-3600-01-001.000", names_to = "PARCEL", values_to = "number_of_high_value_properties_within_mile" )
```


Adding them together

```{r}
Allen_Delinquents_With_Distance_Info <- left_join(Allen_County_Delinquent_Geo, Allen_Distance_Info1,  by="PARCEL")
Allen_Delinquents_With_Distance_Info <- left_join(Allen_Delinquents_With_Distance_Info, Allen_Distance_Info2,  by="PARCEL")
```

Which properties were just near expensive properties, period?
```{r}
Allen_Delinquents_With_Distance_Info  <- Allen_Delinquents_With_Distance_Info %>% 
  mutate(Within_half_mile=if_else(number_of_high_value_properties_within_half_mile>=1,1,0)) %>% 
 mutate(Within_mile=if_else(number_of_high_value_properties_within_mile>=1,1,0))          
```




And since we have it, let's look at the date it was certified delinquent

```{r}
Allen_Delinquents_With_Distance_Info$Year_Delinquent <- Allen_Delinquent$`CertDlq Year`[match(Allen_Delinquents_With_Distance_Info$PARCEL,Allen_Delinquent$PARCEL)]
```

And let's measure that in time

```{r}
Allen_Delinquents_With_Distance_Info <- Allen_Delinquents_With_Distance_Info %>% 
  mutate(Years_Since_Delinquent=2021-Year_Delinquent)
```


Which are land bank properties?

```{r}
Allen_Delinquents_With_Distance_Info <- Allen_Delinquents_With_Distance_Info %>% 
  mutate(Gov_Foreclosure=if_else(PARCEL %in% Allen_County_LB_Properties$PARCEL, 1,0))
```


How much do people owe as a percentage of what their place is worth? This ended up not working out because we don't know how much properties owed when they went into the land bank or when they were certified tax delinquent. 





And now, the machine learning part. 


```{r}
stack_overflow <- as.data.frame(Allen_Delinquents_With_Distance_Info)


stack_overflow <- stack_overflow %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor) %>% 
   mutate(Within_mile = factor(Within_half_mile, levels = c(1,0)))


# Create stack_select dataset
stack_select <- stack_overflow %>%
  #select(Gov_Foreclosure, owed_as_percentage_of_lot_value, number_of_high_value_properties_within_500m, number_of_high_value_properties_within_1000m, Sum_of_distance_to_high_value_properties) 
    select(MKTTOTVAL, Gov_Foreclosure)

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```


