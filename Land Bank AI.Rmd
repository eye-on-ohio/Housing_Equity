---
title: "Land Bank AI"
author: "Lucia Walinchus"
date: "Summer 2021"
output: html_document
---

```{r setup, include=TRUE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(lubridate)
library(DT)
library(ggmap)
library(XML)
library(purrr)
library(leaflet)
library(sf)
library(readxl)
library(tidymodels)
library(themis)

```



We got this from a FOIA from the Cuyahoga County Fiscal Office

```{r cars}
TransferHistory <- rio::import("TRANSFER_HISTORY_CUYAHOGA_COUNTY.txt") # Warning, after you start this, you should probably go to lunch. This takes a ridiculously long time to load. 
```

Oh boy ~2.8 million transfers! 

```{r }
#Date is a Character, and we need the recorded date as a date
TransferHistory$Transfer_date <- dmy(TransferHistory$TRANSFER_DATE)

```




County Land Banks (not to be confused with city land banks) have only been around though since 2009, and so we will only need that. 

For reference- SB 353 analysis by the Legislative Service Commission: https://www.lsc.ohio.gov/documents/gaDocuments/analyses127/08-sb353-127.pdf

_"Authorizes  a  county  with  a  population  exceeding  1.2  million  to  form,  within  one  year  of  the  act's  effective  date,  a  county  land  reutilization  corporation   (CLRC),   a   nonprofit   corporation,   for   the   purposes   of   promoting  development  and  managing  and  facilitating  the  reclamation,  rehabilitation,  and  reutilization  of  vacant,  abandoned,  tax-foreclosed,  or  other real property."_
And the effective date is April 7, 2009.

[Side note, really weird, the date function reads "68" as 2068" not "1968." For our purposes, we are only going to look at county land banks, as city land banks don't really get the party started.And that doesn't happen until 2009. But that's why there are several errors that come up as dated in the future.]


```{r}
Transfer_History_Post_April09 <-filter(TransferHistory, Transfer_date>"2009-04-06"&Transfer_date<"2021-12-31")
rm(TransferHistory)# To give your computer memory a break
```



Through  additional FOIAs:

Note: we removed 16 listings of bad data. Which isn't bad for 48,852 transactions in 2020, but still not ideal.

```{r}
Cuyahoga2020 <- rio::import("ENTIRE COUNTY_SalesListing_2020_EntireYear.xlsx")


setwd("~/Code/Blue/Housing_Equity/Cuyahoga 2021 Transfers/")

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.txt$", 
                            full.names = TRUE)


Cuyahoga2021 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_csv, progress = show_progress())

```

Notes: 006-02-013 in 2020 had an error causing an extra column. This property is never taken over by the land bank so excluded for our purposes




Putting them together and in the same format: 

```{r}
Cuyahoga2020_2021 <- rbind(Cuyahoga2020,Cuyahoga2021)

Cuyahoga2020_2021 <- Cuyahoga2020_2021 %>% 
  rename(PROPERTY_NUMBER=Parcel, 
         Transfer_date=`Sale Date`,
         GRANTOR1=Grantor,
         GRANTEE1=Grantee,
         DEED_TYPE=`Deed Type`,
         AUTO_FILE_NUMBER=AFN) %>% 
  select(PROPERTY_NUMBER, 
         Transfer_date,
         GRANTOR1,
         GRANTEE1,
         DEED_TYPE,
         AUTO_FILE_NUMBER)


Transfer_History_Post_April09 <- Transfer_History_Post_April09 %>% 
  select(PROPERTY_NUMBER, 
         Transfer_date,
         GRANTOR1,
         GRANTEE1,
         DEED_TYPE,
         AUTO_FILE_NUMBER) %>% 
  rbind(Cuyahoga2020_2021)

```






How many properties are going to and from land banks? Or were forfeited to the state?

Going TO landbanks
```{r}
Land_Bank_Transfers <- Transfer_History_Post_April09 %>% 
  filter(str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF"))
summary(Land_Bank_Transfers)
```
From Land banks to others. (Note City and County have many between them.)
```{r}
Land_Bank_Recipients <- Transfer_History_Post_April09 %>% 
  filter(str_detect(GRANTOR1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF"))
summary(Land_Bank_Recipients)
```


How did Covid affect transfers? 
```{r}
Land_Bank_Transfers %>% 
  count(year(Transfer_date))
```

Fewer, also affected by litigation. 

```{r}
Land_Bank_Recipients %>% 
  count(year(Transfer_date))
```




Who got the most land bank transfers? 

```{r}
Largest_Land_Bank_Recipients <- Land_Bank_Recipients %>% 
  group_by(GRANTEE1) %>% 
  summarize(Total=n())

datatable(Largest_Land_Bank_Recipients)


```





How many properties went from one to another? 

Note: we can't just look for stuff with "Land" in it since "land" is in "Cleveland."


```{r}
To_and_from_Land_Banks <- rbind(Land_Bank_Recipients, Land_Bank_Transfers) %>% 
  filter(str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization") & str_detect(GRANTOR1, "land bank|LAND BANK|REUTILIZATION|Reutilization")) %>% 
  distinct()
  


```


These are the properties that went from one land bank to another. This is pretty common as county land banks came later and had more control over things. 

```{r}
Properties_That_Switched_Land_Banks <- Transfer_History_Post_April09 %>% 
  filter(PROPERTY_NUMBER %in% To_and_from_Land_Banks$PROPERTY_NUMBER)

```


Where exactly are these properties? First, let's bring in our parcel listing. 


Getting unique land banks properties
```{r}

Land_Bank_Dups <- Land_Bank_Transfers %>% 
  group_by(PROPERTY_NUMBER) %>% 
  summarize(Total=n()) 
  

datatable(Land_Bank_Dups)

```

Some parcels went to the land bank multiple times. Or were transferred between land banks. So for our purposes, we want to make sure they only come up once. 

```{r}
#Land_Bank_Unique  <- Land_Bank_Transfers %>% 
 #distinct(PROPERTY_NUMBER, .keep_all = TRUE)
```

Now where are all these properties? 
To locate them, we will need the list of all parcels, to match the IDs: 

```{r}
Cuyahoga_Properties <- rio::import("CUYAHOGA_PARCELLISTING_2019.csv") #also from a FOIA
```

And we're going to merge them by the property number. Note the parcel id and property number are the same except the property number has dashes. 

```{r}
Cuyahoga_data <- left_join(Land_Bank_Transfers, Cuyahoga_Properties, by = "PROPERTY_NUMBER")
```


Here's a challenge: the *property* address isn't always the *mailing* address. 

Second challenge: Some of these properties were demolished such as parcel number 015-06-060 which went through the land bank, was demolished, and now that property doesn't exist. So there is no property number. This is actually exactly what land banks are supposed to do! But this makes our job a lot harder. 


```{r}
#Cuyahoga_demolished <- Cuyahoga_data %>% 
  #filter(is.na(PROPERTY_NUMBER.x))

```
Not in the code: We found each address by hand. 
![This involved going through hundreds of old property records, maps, and approximating where these properties were located.]("example of going through property photos- Cuyahoga.png")

In addition, there are approximately with 1,937 parcels with a property number that still exists but with no address. These are empty lots. 

To map this data, we used parcel shapefiles from [Cuyahoga County's Open Data Portal.](https://data-cuyahoga.opendata.arcgis.com/) Specifically, we want to open and combine the shapefiles for Cleveland and non-Cleveland so we can map this and just get an idea of where these properties are. 

```{r}
setwd("~/Code/Blue/Housing_Equity/Combined_Parcels_-_Non-Cleveland_Only")
Non_Cleveland_Geo <- sf::st_read("Combined_Parcels_-_Non-Cleveland_Only.shp")
head(Non_Cleveland_Geo)

setwd("~/Code/Blue/Housing_Equity/Combined_Parcels_-_Cleveland_Only")
Cleveland_Geo <- sf::st_read("Combined_Parcels_-_Cleveland_Only.shp")
head(Non_Cleveland_Geo)

Cuyahoga_geo <- rbind(Non_Cleveland_Geo, Cleveland_Geo)
```

This is awesome but for some reason the column names are not the same as the website, so fixing that. 

```{r}
Cuyahoga_geo <- Cuyahoga_geo %>% 
  rename(parcel_year=parcel_yea,
         Transfer_Date=transfer_d,#this was causing an issue
         Tax_Legal_Use_Description=tax_luc_de,
         Property_Class=property_c,
         Certified_Tax_Legal_Use_Classification=certified_,
         Certified_Tax_Land=certifie_1,
         Certified_Tax_Building=certifie_2,
         Certified_Tax_Total=certifie_3,
         Certified_Exempt_LUC=certifie_4,
         Certified_Exempt_Land=certifie_5,
         Certified_Exempt_Building=certifie_6,
         Certified_Exempt_Total=certifie_7,
         Certified_Abated_LUC=certifie_8,
        Certified_Abated_Land=certifie_9,
          Certified_Abated_Building=certifie10,
          Certified_Abated_Total=certifie11,
        Gross_Certified_Land=gross_cert,
         Gross_Certified_Building=gross_ce_1,
         Gross_Certified_Total=gross_ce_2,
         Residential_building_count=res_bldg_c,
         Total_residential_Living_Area=total_res_,
         Total_residential_rooms=total_re_1,
         Commerical_Building_Count=com_bldg_c,
         Total_Commerical_UseArea=total_com_)
```





```{r}
Cuyahoga_data_with_map <- merge(Cuyahoga_data, Cuyahoga_geo, by.x="PARCEL_ID",by.y="parcel_id")
```

Exporting this to vizualize it
```{r vizualizing it}
Cuyahoga_data_with_map <- st_as_sf(Cuyahoga_data_with_map)

plot(Cuyahoga_data_with_map %>% select(PARCEL_ID, geometry))

```

Adding a background map so we can zoom in and out.
```{r}
Cuyahoga_Properties_plotted <- leaflet(data = Cuyahoga_data_with_map) %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons()
  
Cuyahoga_Properties_plotted
```


Now we need to bring in the parcel numbers that no longer exist. (This was done manually, by going through the chain of title.)

So this is two different types of map data: parcels and points. Since these parcels no longer exist, we can't get their polygons, so we had to find their addresses and then geocode them to get their points.  For visualization purposes, we will want to see where these are. And some addresses have more than one property number. (And some property numbers have more than one address, as in apartments.) 

So we also need the version with the property numbers for the machine learning part. 

Bringing in the demolished parcels with approximate addresses
```{r}
Cuyahoga_demolished_with_Addresses <- rio::import("Cuyhoga Properties demolished with approximate addresses.csv")
```

Bringing in the geocoded parcels

```{r}
Cuyahoga_demolished_with_Addresses_and_Geocoded <- rio::import("Cuyahoga demolished with addresses and geocoded.csv")

```

Adding the geocoded parcels to the map:
```{r}
Cuyahoga_Properties_plotted_all <- leaflet(data = Cuyahoga_data_with_map) %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons() %>% 
  addMarkers(lat = Cuyahoga_demolished_with_Addresses_and_Geocoded$lat, lng = Cuyahoga_demolished_with_Addresses_and_Geocoded$lon, popup = "address")
  
Cuyahoga_Properties_plotted_all
```

Some parcels were subsumed into other parcels that had never been taken over by the land bank, some became part of other land bank properties, etc. 


Where are political donors in relation to this map? Note: Some listed home and some listed their work adddress. 

```{r}
setwd("~/Code/Blue/Housing_Equity/Political Contributions")

Cuyahoga_Campaign_Contributions <- rio::import("County Campaign Contributions Cuyahoga_geocoded_v3.0.csv")
Cuyahoga_Campaign_Contributions$Amount <- as.double(Cuyahoga_Campaign_Contributions$Amount)

```

```{r}
Cuyahoga_Properties_plotted_with_Contributions <- leaflet() %>% 
  addTiles(data = Cuyahoga_data_with_map) %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(data = Cuyahoga_data_with_map) %>% 
  addMarkers(lat = Cuyahoga_demolished_with_Addresses_and_Geocoded$lat, lng = Cuyahoga_demolished_with_Addresses_and_Geocoded$lon, popup = Cuyahoga_demolished_with_Addresses_and_Geocoded$address) %>% 
  addCircleMarkers(lat = Cuyahoga_Campaign_Contributions$lat, lng =Cuyahoga_Campaign_Contributions$lon, color ="red", popup =Cuyahoga_Campaign_Contributions$address)
  
Cuyahoga_Properties_plotted_with_Contributions
```

Doesn't look like there's much overlap betwen political contributors of Budish and Gallagher and land bank properties. Though note: in Cuyahoga county, nine people actually adjudicate the BOR forclosures, not the BOR itself. We got that list of people through 



What properties were behind on their taxes an potentially subject to being taken over?


```{r}
Cuyahoga_Delinquent_2018 <- rio::import("DelnqntTaxPayers_Entire County JAN 23 2019.xlsx")#they have to be certified in January that they didn't pay all of 2018

setwd("~/Code/Blue/Housing_Equity/Cuyahoga_Delinquent_2019")

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.xlsx$", 
                            full.names = TRUE)

Cuyahoga_Delinquent_2019 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_excel, progress = show_progress())

setwd("~/Code/Blue/Housing_Equity/Cuyahoga_Delinquent_2020")

Cuyahoga_Delinquent_2019 <- Cuyahoga_Delinquent_2019 %>% filter(SCHOOL_DESCR!="Delinquent is defined as having a balance greater than $0.00 and no payment plan  after 2nd half collection closed.CUYAHOGA COUNTY ASSUMES NO LIABILITY FOR DAMAGES AS A RESULT OF ERRORS, OMISSIONS OR DISCREPANCIES CONTAINED IN THESE PAGES. PROSPECTIVE PURCHASERS SHOULD CONSULT A REAL ESTATE ATTORNEY AND PURCHASE A TITLE INSURANCE POLICY PRIOR TO THE SALE.") %>% filter(!is.na(SCHOOL_DESCR))

list_of_files <- list.files(path = ".", recursive = TRUE,
                            pattern = "\\.xlsx$", 
                            full.names = TRUE)

Cuyahoga_Delinquent_2020 <- list_of_files %>%
  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%
  purrr::map_df(read_excel, col_types ="text", progress = show_progress())


Cuyahoga_Delinquent_2020 <- Cuyahoga_Delinquent_2020 %>% filter(SCHOOL_DESCR!="Delinquent is defined as having a balance greater than $0.00 and no payment plan  after 2nd half collection closed.CUYAHOGA COUNTY ASSUMES NO LIABILITY FOR DAMAGES AS A RESULT OF ERRORS, OMISSIONS OR DISCREPANCIES CONTAINED IN THESE PAGES. PROSPECTIVE PURCHASERS SHOULD CONSULT A REAL ESTATE ATTORNEY AND PURCHASE A TITLE INSURANCE POLICY PRIOR TO THE SALE.") %>% filter(!is.na(SCHOOL_DESCR))
```




Adding those together

```{r}
Cuyahoga_Delinquent_2018 <- Cuyahoga_Delinquent_2018 %>% 
  rename(SCHOOL_DESCR=SCHOOL_DIST_NAME) %>% 
  select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquent_2019 <- Cuyahoga_Delinquent_2019 %>% select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquent_2020 <- Cuyahoga_Delinquent_2020 %>% select(PROPERTY_NUMBER,SCHOOL_DESCR,PRIMARY_OWNER_NAME,TAX_YEAR,CLASSIFICATION_ID,CLASSIFICATION_DESCR, GRAND_TOTAL_OWED, GRAND_TOTAL_PAID, GRAND_TOTAL_BALANCE,UPDATE_DATE)

Cuyahoga_Delinquents <- rbind(Cuyahoga_Delinquent_2018, Cuyahoga_Delinquent_2019,Cuyahoga_Delinquent_2020)
head(Cuyahoga_Delinquents)

```



Some properties might be delinquent both years. 
```{r}
  Cuyahoga_Delinquents %>% count(PROPERTY_NUMBER)
```

So we need to keep only the distinct ones

```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>%  distinct(PROPERTY_NUMBER, .keep_all = TRUE)
```




That's a lot of properties. 

It appears that not all those properties are actually delinquent though. For example, 120-19-003 is a dorm at Case Western reserve. 

We asked them why they owed ~$4 million in taxes, they said they were exempt. 
!("Case Western taxes.jpeg")

Here is a reference to the [Legal Use Codes in Ohio.]("https://codes.ohio.gov/ohio-administrative-code/rule-5703-25-10")
So though their Classification ID is in the commercial taxable bracket, In the (400-0s) their exempt Land Use Code makes them exempt from tax. (In the 600-0s)

So we need to determine what the exempt legal use code is, and filter that out. 
note: properties in the 700-0s are exempt because they are in the land bank essentially 

```{r}
Cuyahoga_Delinquents$CLASSIFICATION_ID <- as.double(Cuyahoga_Delinquents$CLASSIFICATION_ID)

Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  filter(CLASSIFICATION_ID<6000|CLASSIFICATION_ID>6999|is.na(CLASSIFICATION_ID))
```




Which parcels were eventually foreclosed upon?
First, our main chunk of properties. Then, the property numbers that were demolished, which were all LB properties.

We want to make sure that we get the folks who were late on their taxes and THEN the government chose to foreclose on them. (So we don't tage anyone who got their property from the land bank and then was late on their taxes.) 

You have to not pay your taxes for a whole year in Ohio, then they are certified late by the county Auditor. So Tax Year 2018 means your delinquent certification was actually issued in January of 2019. 

So for our purposes, we want to look for property transfers to the land bank which took place after January 23, 2019. 


```{r}
Cuyahoga_data_with_map_post_Jan_2019 <- filter(Cuyahoga_data_with_map, Transfer_date>"2019-01-23")

Cuyahoga_demolished_with_Addresses$Transfer_date <- lubridate::dmy(Cuyahoga_demolished_with_Addresses$TRANSFER_DATE)
Cuyahoga_demolished_with_Addresses_post_Jan_2019 <- filter(Cuyahoga_demolished_with_Addresses, Transfer_date>"2019-01-23")

Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(Land_Bank=if_else(PROPERTY_NUMBER %in% Cuyahoga_data_with_map_post_Jan_2019$PROPERTY_NUMBER|PROPERTY_NUMBER %in% Cuyahoga_demolished_with_Addresses_post_Jan_2019$PROPERTY_NUMBER,1,0))

```

 

How many is that? What percent of 2018-2019-2020 delinquent properties were taken over by the government or land bank in 2019, 2020, or 2021?

```{r}
sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)
nrow(Cuyahoga_Delinquents)

sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)/(sum(Cuyahoga_Delinquents$Land_Bank, na.rm = TRUE)+
nrow(Cuyahoga_Delinquents))#Order of operations
```


And putting the amount owed in numeric form: 

```{r}
Cuyahoga_Delinquents$GRAND_TOTAL_OWED <- as.double(Cuyahoga_Delinquents$GRAND_TOTAL_OWED)
Cuyahoga_Delinquents$GRAND_TOTAL_PAID <- as.double(Cuyahoga_Delinquents$GRAND_TOTAL_PAID)
Cuyahoga_Delinquents$GRAND_TOTAL_BALANCE <-as.double(Cuyahoga_Delinquents$GRAND_TOTAL_BALANCE)
```


The value of most land bank properties is hard to gauge. Some are worth essentially negative money, as they would be harder to fix up than to actually operate. 

Some of the people who are behind on their taxes aren't behind by much- for example, the 169 people who owe a cent- that's probably a typo. 


On the other had, a property in Montgomery county got foreclosed upon [for owing thirteen cents.](https://eyeonohio.com/wp-content/uploads/2020/02/Affidavit-of-Treasurer-for-parcel-number-13508151.pdf) They hadn't actually paid their tax bill in years, but they put that amount on the lien as the actual amount didn't matter- it's not like they would be collecting it anyway, they just needed to get a certified lien to foreclose upon. 

So to get a sense of how behind these properties are on their taxes, we will calculate the amount owed as a percentage of their certified total property value. 

```{r}
Cuyahoga_Delinquents$Certified_Total_Value <- Cuyahoga_Properties$CERT_TOT[match(Cuyahoga_Delinquents$PROPERTY_NUMBER, Cuyahoga_Properties$PROPERTY_NUMBER)]

Cuyahoga_Delinquents <-  Cuyahoga_Delinquents %>% 
  mutate(owed_as_percentage_of_lot_value=GRAND_TOTAL_BALANCE/Certified_Total_Value)
```

Note: 68 properties are listed as having a value of 0. This is on the website too, and there doesn't seem to be a clear pattern. We are adding this to our list to ask the fiscal officer. 

```{r}
Cuyahoga_Delinquents_with_0_value <- Cuyahoga_Delinquents %>% select(PROPERTY_NUMBER,Certified_Total_Value) %>% filter(Certified_Total_Value==0)

Cuyahoga_Delinquents <-  Cuyahoga_Delinquents %>% filter(Certified_Total_Value!=0)
```




So 2,362 properties went to the government, but that's only 2% of the properties. (And some ostensibly people paid their back taxes!) 

Next: we need to bring in the categories of each school district. 

This is from the Ohio Department of Education. [There's no direct download link, you have to choose what you want.]
)

They don't allow you to download all the info at once by district, or if they do we couldn't figure it out. But anyway, this takes every school in Cuyahoga. (So we searched for "Cuyahoga," clicked on the button next to each one, then downloaded.) 


```{r}
Cuyahoga_schools_demographic <- rio::import("Cuyahoga Schools Info.xlsx")

head(Cuyahoga_schools_demographic)
```

A few things about this data: 
*They write "<10" for small categories less than 10, instead of an actual number. 
*Unlike on the website, this has decimals. How do you have part of a kid? Clarification from the Ohio Dept of education's Mandy Minick: "we count students as full time equivalents (FTEs) based on the length of time they are enrolled.  A student enrolled all year counts as 1.0 FTE in the total.  A student enrolled for half a year is 0.5 FTEs and so on.  We sum all the FTEs across the entire year (hence, the decimals). The spreadsheet on the left shows the decimals (33.188847), while the one on the right is rounded to the closest whole number for ease of reading."


How many kids are in each district?

```{r}
Cuyahoga_schools_demographic_no_remainders <- Cuyahoga_schools_demographic %>% 
  filter(CountOfEnrolledStudents!="<10")

  
Cuyahoga_schools_demographic_no_remainders$CountOfEnrolledStudents <- as.double(Cuyahoga_schools_demographic_no_remainders$CountOfEnrolledStudents)

Cuyahoga_school_pop <- Cuyahoga_schools_demographic_no_remainders%>% 
  group_by(District) %>% 
    summarize(Total=sum(CountOfEnrolledStudents))

datatable(Cuyahoga_school_pop)
```

Here we are going to treat race and socioeconomic status as factors, even though we have numbers. Why? 

These variables are going to affect our data in a categorical way. You either are discriminating on race or class or you are not. The amount of discrimination does not depend on the amount of people. For example, in the [Kaggle Titantic competition](https://www.kaggle.com/c/titanic) (which is great way to learn machine learning!) The passengers' ticket price is listed. Which passengers were more likely to survive?

You are more likely to survive _if you have a first class ticket_. Period. If your first class ticket costs twice as much as the next first class ticket, that didn't make you twice as likely to survive as someone with a ticket half that price. 

Which school districts have large minority populations? Which have populations with a high number of kids in poverty? 


Finding percentage of minority by District: 
```{r}
Cuyahoga_schools_by_race <- Cuyahoga_schools_demographic_no_remainders %>% 
  group_by(District,`Race/Ethnicty`,) %>% #typo in original for 'ethnicity'
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Cuyahoga_schools_by_race)
```

```{r}
Cuyahoga_schools_by_race %>% 
  filter(`Race/Ethnicty`=="White, Non-Hispanic") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```




Finding percent disadvantaged by district: 
 
```{r}
Cuyahoga_schools_by_poverty <- Cuyahoga_schools_demographic_no_remainders %>% 
  group_by(District,Subgroup,) %>% 
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Cuyahoga_schools_by_poverty)
```

```{r}
Cuyahoga_schools_by_poverty %>% 
  filter(Subgroup=="Economic Disadvantaged") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```


And categorizing them: 

```{r}
Cuyahoga_schools_by_race_with_majority_white <- Cuyahoga_schools_by_race %>% 
  group_by(District) %>% 
summarize(Majority_White=if_else(`Race/Ethnicty`=="White, Non-Hispanic"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_White_District=if_else(sum(Majority_White)==1,1,0))
  

Cuyahoga_schools_with_majority_disadvantaged <- Cuyahoga_schools_by_poverty %>% 
  group_by(District) %>% 
summarize(Majority_Disadvantaged=if_else(Subgroup=="Economic Disadvantaged"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_Disadvantaged_District=if_else(sum(Majority_Disadvantaged)==1,1,0))

  
```

How are the school districts represented in our delinquent dataset?

```{r}
#Cuyahoga_Delinquents_by_School_District <- Cuyahoga_Delinquents %>% group_by(School_District) %>% summarize(Total=n())
#datatable(Cuyahoga_Delinquents_by_School_District)

```



The names of school districts is the same but listed a bit differently in both data sets, so we have to do a bit of manual matching here. 

```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(School_Dest_Standardized= case_when(
     SCHOOL_DESCR=="CLEVELAND MSD" ~ "Cleveland Municipal - 043786 (Cuyahoga)",
     SCHOOL_DESCR=="CLEVELAND CSD" ~ "Cleveland Municipal - 043786 (Cuyahoga)", #Not sure why this is in here two ways but properties on the website listed as MSD here are CSD on Auditor's site
     SCHOOL_DESCR=="BEREA CSD" ~ "Berea City - 043612 (Cuyahoga)",                              
SCHOOL_DESCR=="SHAKER HEIGHTS CSD"  ~   "Shaker Heights City - 044750 (Cuyahoga)",                 
SCHOOL_DESCR=="BAY VILLAGE CSD" ~ "Bay Village City - 043547 (Cuyahoga)" ,                       
SCHOOL_DESCR=="WESTLAKE CSD"~   "Westlake City - 045062 (Cuyahoga)",                        
SCHOOL_DESCR=="NORTH OLMSTED CSD" ~ "North Olmsted City - 044529 (Cuyahoga)",                     
SCHOOL_DESCR=="OLMSTED FALLS CSD" ~"Olmsted Falls City - 046573 (Cuyahoga)",                     
SCHOOL_DESCR=="ROCKY RIVER CSD"~  "Rocky River City - 044701 (Cuyahoga)",                      
SCHOOL_DESCR=="LAKEWOOD CSD" ~  "Lakewood City - 044198 (Cuyahoga)",                        
SCHOOL_DESCR=="FAIRVIEW PARK CSD"  ~ "Fairview Park City - 043976 (Cuyahoga)",                     
SCHOOL_DESCR=="STRONGSVILLE CSD"~ "Strongsville City - 044842 (Cuyahoga)" ,                     
SCHOOL_DESCR=="BROOKLYN CSD" ~  "Brooklyn City - 043653 (Cuyahoga)",                         
SCHOOL_DESCR=="PARMA CSD"~ "Parma City - 044636 (Cuyahoga)",                              
SCHOOL_DESCR=="NORTH ROYALTON CSD"  ~  "North Royalton City - 044545 (Cuyahoga)",                  
SCHOOL_DESCR=="BRECKSVILLE-BROADVIEW HEIGHTS SD"  ~  "Brecksville-Broadview Heights City - 043646 (Cuyahoga)",  
SCHOOL_DESCR=="BRECKSVILLE-BROADVIEW HEIGHTS" ~  "Brecksville-Broadview Heights City - 043646 (Cuyahoga)", 
SCHOOL_DESCR=="CUYAHOGA HEIGHTS LSD"~  "Cuyahoga Heights Local - 046557 (Cuyahoga)",           
SCHOOL_DESCR=="GARFIELD HEIGHTS CSD" ~  "Garfield Heights City Schools - 044040 (Cuyahoga)" ,                
SCHOOL_DESCR=="INDEPENDENCE LSD" ~ "Independence Local - 046565 (Cuyahoga)",                       
SCHOOL_DESCR=="EUCLID CSD"~  "Euclid City - 043950 (Cuyahoga)",                            
SCHOOL_DESCR=="RICHMOND HEIGHTS LSD" ~   "Richmond Heights Local - 046599 (Cuyahoga)",               
SCHOOL_DESCR=="SOUTH EUCLID-LYNDHURST CSD" ~ "South Euclid-Lyndhurst City - 044792 (Cuyahoga)",            
SCHOOL_DESCR=="EAST CLEVELAND CSD"~ "East Cleveland City School District - 043901 (Cuyahoga)",                     
SCHOOL_DESCR=="CLEVELAND HEIGHTS-UNIVERSITY HEIGHTS SD"~"Cleveland Heights-University Heights City - 043794 (Cuyahoga)",
SCHOOL_DESCR=="CLEVELAND HTS-UNIVERSITY HTS C"~"Cleveland Heights-University Heights City - 043794 (Cuyahoga)", #also written two ways
SCHOOL_DESCR=="WESTLAKE CSD" ~ "Westlake City - 045062 (Cuyahoga)",
SCHOOL_DESCR=="BEACHWOOD CSD"~  "Beachwood City - 043554 (Cuyahoga)",                         
SCHOOL_DESCR=="WARRENSVILLE HEIGHTS CSD"~ "Warrensville Heights City - 045005 (Cuyahoga)",               
SCHOOL_DESCR=="ORANGE CSD" ~  "Orange City - 046581 (Cuyahoga)",                          
SCHOOL_DESCR=="MAPLE HEIGHTS CSD"~  "Maple Heights City - 044305 (Cuyahoga)",                     
SCHOOL_DESCR=="BEDFORD CSD"  ~  "Bedford City - 043562 (Cuyahoga)",                         
SCHOOL_DESCR=="MAYFIELD CSD" ~ "Mayfield City - 044370 (Cuyahoga)",                         
SCHOOL_DESCR=="CHAGRIN FALLS EVSD" ~"Chagrin Falls Exempted Village - 045286 (Cuyahoga)",                     
SCHOOL_DESCR=="SOLON CSD" ~ "Solon City - 046607 (Cuyahoga)" ))                            



```

And here we incorporate race and socioeconomic status as factors. 

```{r}
Cuyahoga_Delinquents$is_school_district_majority_white <- Cuyahoga_schools_by_race_with_majority_white$Majority_White_District[match(Cuyahoga_Delinquents$School_Dest_Standardized,Cuyahoga_schools_by_race_with_majority_white$District)]

Cuyahoga_Delinquents$is_school_district_majority_disadvantaged <- Cuyahoga_schools_with_majority_disadvantaged$Majority_Disadvantaged_District[match(Cuyahoga_Delinquents$School_Dest_Standardized,Cuyahoga_schools_with_majority_disadvantaged$District)]


```


Some exploratory analysis: 

Of the parcels taken over by the land bank, how many are in majority white districts? How does that compare the number of delinquent taxpayers who are in majority white districts?
```{r}
prop.table(table(Cuyahoga_Delinquents$is_school_district_majority_white, Cuyahoga_Delinquents$Land_Bank))
```
On the left, in the column, 0 represents the school district is not majority white, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.


What percentage of delinquent properties are in majority white school districts? What percentage of properties that go to the land bank are in majority white school districts?
```{r}

Cuyahoga_Delinquents %>% group_by(Land_Bank) %>% count(is_school_district_majority_white)
```




And majority disadvantaged? How does that compare to the number of delinquent taxpayers who are in majority disadvantaged districts?

```{r}
prop.table(table(Cuyahoga_Delinquents$is_school_district_majority_disadvantaged, Cuyahoga_Delinquents$Land_Bank))
```
On the left, in the column, 0 represents the school district is not majority disadvantaged, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.









Now we're trying to find: are officials more like to give parcels TO people if it's close to a big amenity? (Like a college or hospital?) To "spur development?"




###Machine Learning


Okay step one: choosing an appropriate model. We want to predict here whether a property will end up in the land bank or not. So we are looking to build a Classification model. 

So first, we have very few Land bank properties versus delinquent properties overall. And this, of course, is our whole story. Who has the power to decide where this will happen? But we will have to take this into account and try to split our training and test sets with an equal number of land bank properties. 

Next, we are going to split the dataset into training and testing models, evenly diving the sections between the Land Bank and non-land bank properties. 

```{r}
stack_overflow <- Cuyahoga_Delinquents %>%
    mutate(Land_Bank = factor(Land_Bank, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_DESCR, -School_Dest_Standardized)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Land_Bank)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Land_Bank)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Land_Bank ~ ., data = stack_train) %>% 
    step_downsample(Land_Bank)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Land_Bank, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Land_Bank, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Land_Bank, estimate = .pred_glm)
accuracy(results, truth = Land_Bank, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Land_Bank, estimate = .pred_glm)
ppv(results, truth = Land_Bank, estimate = .pred_tree)
```

Which variables are particularly important? 

```{r}
data <- Cuyahoga_Delinquents %>%
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_DESCR, -School_Dest_Standardized)

names(data)

#writing a function to automate writing the formula: 
n<-names(data)
form1 = as.formula(paste("Land_Bank ~", paste(n[!n %in% "Land_Bank"], collapse = " + ")))
```


Let's see where properties are in relation to future developments. 

Our Land_Bank_Recipients dataframe above takes properties where the land bank is the _grantor_ who then gives the property to the grantee. Legally, city land banks and county land banks are very different. County land banks have more powers, to get rid of tax liens and all other liens. So we want to look specifically at properties that went from the government to another entity (person or company) not just another land bank to hold onto. 


```{r}
Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients %>% 
  filter(!str_detect(GRANTEE1, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF|LAND REUIT|REUT|CITY OF CLEVELAND")) 
summary(Land_Bank_Recipients_Without_interlandbank_transfers)
```


```{r}
Land_Bank_Recipients_By_Type <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  mutate(type=if_else((str_detect(GRANTEE1,",")&!str_detect(GRANTEE1,"LLC")),"person","group"))


ggplot(Land_Bank_Recipients_By_Type)+
  aes(x=year(Transfer_date),fill=type)+
  geom_bar()+
  facet_wrap(~type)

         
```





Where are these properties?

```{r}
#Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
#distinct(PROPERTY_NUMBER, .keep_all = TRUE)

Land_Bank_Recipients_Without_interlandbank_transfers <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))



Land_Bank_Recipients_Without_interlandbank_transfers <- merge(Land_Bank_Recipients_Without_interlandbank_transfers, Cuyahoga_geo, by="parcel_id")
```


color = lubridate::year(Land_Bank_Recipients_Without_interlandbank_transfers$Transfer_date)


Where are the properties that went from a land bank TO a person within the past year?
```{r}
Land_Bank_Recipients_Without_interlandbank_transfers <- st_as_sf(Land_Bank_Recipients_Without_interlandbank_transfers)

Cuyahoga_Land_Bank_Recipient_Properties_plotted2021 <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  filter(year(Transfer_date)==2021) %>% 
  leaflet() %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(popup = Land_Bank_Recipients_Without_interlandbank_transfers$parcel_id)
  
Cuyahoga_Land_Bank_Recipient_Properties_plotted2021
```


How does that compare to properties the Land Bank is holding onto?

```{r}

Land_Bank_Recipients_Without_interlandbank_transfers_20_and_21 <- Land_Bank_Recipients_Without_interlandbank_transfers %>% 
  filter(year(Transfer_date)>2019)


Cuyahoga_Land_Bank_Recipient_Properties_versus_ones_they_held_onto <- Cuyahoga_geo %>% 
  filter(str_detect(deeded_own, "land bank|LAND BANK|REUTILIZATION|Reutilization|FORF")) %>% 
  leaflet() %>% 
  addTiles() %>% 
  setView(-81.66754692563244,41.43053368069701, zoom = 10) %>% 
  addPolygons(popup = Cuyahoga_geo$parcel_id, color = "red") %>% 
  addPolygons(data= Land_Bank_Recipients_Without_interlandbank_transfers_20_and_21, color="green") #%>% 
  #addPolygons(data = filter(Cuyahoga_geo, Gross_Certified_Total>1000000), color = "blue")
  
Cuyahoga_Land_Bank_Recipient_Properties_versus_ones_they_held_onto
  
```


Where are the expensive properties in Cuyahoga county?

What are the values of Cuyahoga parcels?

```{r}
#ggplot(Cuyahoga_geo)+
  #aes(x=Gross_Certified_Total)+
  #geom_histogram(binwidth = 10000000)+
  #stat_bin(binwidth= 10000000, geom="text", aes(label=..count..), vjust = -1) + 
#scale_x_continuous(breaks = seq(0 , 702599700, 10000000) , labels = comma)+
#theme(axis.text.x = element_text(angle = 45, vjust = 1.2, hjust = 1.1))
  #geom_text(data=Cuyahoga_geo, mapping=aes(x=Gross_Certified_Total),label=Gross_Certified_Total)

#hist(Cuyahoga_geo$Gross_Certified_Total, xlim=range(1000000),labels = TRUE)
```




We are going to set the (rather arbitrary) cutoff of $10 million parcels. 

```{r}
Cuyahoga_geo_millions <- filter(Cuyahoga_geo, Gross_Certified_Total>10000000)
```

How close are gov owed parcels to prime real estate? 

Adding mapping data to Cuyahoga Delinquents
```{r}

Cuyahoga_Delinquents_Geo <- Cuyahoga_Delinquents %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))

Cuyahoga_Delinquents_Geo$geometry <- Cuyahoga_geo$geometry[match(Cuyahoga_Delinquents_Geo$parcel_id,Cuyahoga_geo$parcel_id)]

Cuyahoga_Delinquents_Geo <- Cuyahoga_Delinquents_Geo %>% 
filter(CLASSIFICATION_ID<6000|CLASSIFICATION_ID>6999|is.na(CLASSIFICATION_ID)) 

#raster::crs(Cuyahoga_Delinquents_Geo) <- "EPSG:4326" 
#shape_proj<-st_transform(Cuyahoga_Delinquents_Geo, CRS("+proj=gnom +lat_0=90 +lon_0=-50"))


```
Figuring out how far apart they are

```{r }
library(lwgeom)

sf::sf_use_s2(FALSE)

#Cuyahoga_Distance_Matrix <- as.data.frame(st_distance(Cuyahoga_Delinquents_Geo$geometry,Cuyahoga_geo_millions$geometry)) #This takes quite a while just FYI
#the above line of code is how you would actually do this, but since it take like three days and makes a file that's 10.5 gigs, for the html knit we're just going to pull in the file we alread made here

Cuyahoga_Distance_Matrix <- rio::import("Cuyahoga_Distance_Matrix.csv")
Cuyahoga_Distance_Matrix1 <- rio::import("Cuyahoga_Distance_Matrix1.csv")
#%>% 
  #Sets column names
  
#Cuyahoga_Distance_Matrix1 <- Cuyahoga_Distance_Matrix %>% 
  #`colnames<-`(Cuyahoga_geo_millions$parcel_id) %>% 
  
  #Adds a column containing names so that each row now also has a name
  #cbind(name =Cuyahoga_Delinquents_Geo$PROPERTY_NUMBER ) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.

```

Problem: 
> Cuyahoga_Delinquents_Geo$geometry[[74]]
MULTIPOLYGON EMPTY
> Cuyahoga_Delinquents_Geo$geometry[[73]]
MULTIPOLYGON (((-81.75264 41.48498, -81.75272 41.48475, -81.75307 41.48481, -81.75325 41.48485, -81.75318 41.48507, -81.75264 41.48498)))
Looks like Cuyahoga_Delinquents_Geo$geometry
Geometry set for 96236 features  (with 7024 geometries empty)
It appears that some parcels such as 001-15-805C have no polygon because they are condos. 
And therefore st_distance(Cuyahoga_Delinquents_Geo$geometry[[74]],Cuyahoga_geo_millions$geometry[[1]]) yields NA. 

However, that only means 4,685,008 of 601,285,648 distances couldn't be computed, or 0.007791651, less than 1%. So we are going to exclude them from our calculations, but wanted to note that. 

```{r}
#Cuyahoga_Distance_Matrix <- Cuyahoga_Distance_Matrix %>% 
  #filter(!is.na(V1))

#Cuyahoga_Distance_Matrix1 <- Cuyahoga_Distance_Matrix1 %>% 
  #filter(!is.na(`12123003`))

```


Also, this means that 7024 properties are  not available for analysis, which is about 7% of the properties behind on their taxes. But none of these properties ended up in the Land Bank. 

```{r}
Empty_Delinquents <- Cuyahoga_Delinquents_Geo %>% filter(st_is_empty(geometry))

sum(Empty_Delinquents$Land_Bank)
```

Looks like none of these properties end up the the Cuyahgoa GIS database because they are associated with other parcels. 






Okay now let's do some math to add distance to our machine learning set. 


First, let's add the parcel number
```{r}
Cuyahoga_Delinquents <- Cuyahoga_Delinquents %>% 
  mutate(parcel_id=(sub("-","",PROPERTY_NUMBER,fixed = TRUE))) %>% 
  mutate(parcel_id=(sub("-","",parcel_id,fixed = TRUE)))
```


Dropping the meters units. Very cool that this is in meters but as we don't need to convert anything it's just making things more complicated.
```{r}
library(units)
Cuyahoga_Distance_Matrix_No_Units <- drop_units(Cuyahoga_Distance_Matrix1)

```
Finding the distance to all high-value parcels in the county. You can also do this with janitor's adorn_totals but we want to do a few Similar things so this is easier. 


Finding the number of high-value parcels within 500 meters of the delinquent properties. (Approximately the distance between Al Jenkins' property and the hospital.) 

```{r}
Number_Near <- Cuyahoga_Distance_Matrix_No_Units %>%                                                                                  
  dplyr::summarise_all(~sum(.<500, na.rm = TRUE))
```

And for comparison, also the number twice as far. 

```{r}
Number_Semi_Near <- Cuyahoga_Distance_Matrix_No_Units %>%                                                                           
  dplyr::summarise_all(~sum(.<1000, na.rm = TRUE))
```

```{r}
Cuyahoga_Distance_Info1 <-Number_Near %>% 
  select(-name) %>% 
  pivot_longer("001-02-033":"215-17-001", names_to = "PROPERTY_NUMBER", values_to = "number_of_high_value_properties_within_500m" )
```

```{r}
Cuyahoga_Distance_Info2 <-Number_Semi_Near %>% 
  select(-name) %>% 
  pivot_longer("001-02-033":"215-17-001", names_to = "PROPERTY_NUMBER", values_to = "number_of_high_value_properties_within_1000m" )
```


```{r}
Cuyahoga_Delinquents_With_Distance_Info <- left_join(Cuyahoga_Delinquents, Cuyahoga_Distance_Info1,  by="PROPERTY_NUMBER")
Cuyahoga_Delinquents_With_Distance_Info <- left_join(Cuyahoga_Delinquents_With_Distance_Info, Cuyahoga_Distance_Info2,  by="PROPERTY_NUMBER")

```


###Machine Learning take two: with distance info

First, classification description didn't seem to be a good factor. Let's look into those: 
Which kids of properties are getting foreclosed  upon?

```{r}
Cuyahoga_Delinquents_Kinds <- Cuyahoga_Delinquents_With_Distance_Info  %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  summarize(Total_Gov__Foreclosure=sum(Land_Bank), Total_Behind=n()) %>% 
  ungroup() %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  mutate(Percent=Total_Gov__Foreclosure/sum(Total_Behind))
            
datatable(Cuyahoga_Delinquents_Kinds)
  
```
Two interesting things here. First, it appears there are 3 Land Bank Descriptions in the use code: 

```{r}
unique(Cuyahoga_Delinquents_With_Distance_Info$CLASSIFICATION_DESCR)
```
"City  (LAND BANKS)", "COUNTY LAND BANK" and "LAND REUTILIZATION (LAND BANKS)". So we have been undercounting some land bank properties when we got rid of duplicates. 

Fixing that. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info <- Cuyahoga_Delinquents_With_Distance_Info %>% 
  mutate(Gov_Foreclosure=ifelse(CLASSIFICATION_DESCR=="City  (LAND BANKS)"|CLASSIFICATION_ID=="COUNTY LAND BANK"|CLASSIFICATION_ID=="LAND REUTILIZATION (LAND BANKS)",1,Land_Bank))
```

Next, it seems no condos or house trailers are getting flagged here, even though they are the fourth and ninth highest in number.  
```{r}

Cuyahoga_Delinquents_Kinds <- Cuyahoga_Delinquents_With_Distance_Info  %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  summarize(Total_Gov__Foreclosure=sum(Gov_Foreclosure), Total_Behind=n()) %>% 
  ungroup() %>% 
  group_by(CLASSIFICATION_DESCR) %>% 
  mutate(Percent=Total_Gov__Foreclosure/sum(Total_Behind))
            
datatable(Cuyahoga_Delinquents_Kinds)
  
```

How much do they owe relative to other properties? 
```{r}
House_Trailers_Owed <- Cuyahoga_Delinquents_With_Distance_Info %>% filter(CLASSIFICATION_DESCR=="House trailer")
Single_Fam_Dwelling_Owed <- Cuyahoga_Delinquents_With_Distance_Info %>% filter(CLASSIFICATION_DESCR=="SINGLE FAMILY DWELLING")

summary(House_Trailers_Owed$owed_as_percentage_of_lot_value)
summary(Single_Fam_Dwelling_Owed$owed_as_percentage_of_lot_value)

```

Next, we are going to take out the properties that are no longer in the map database. It looks like these properties are now listed with other properties. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo <- filter(Cuyahoga_Delinquents_With_Distance_Info, !is.na(number_of_high_value_properties_within_500m))
```

What's the rough distribution?

```{r}
ggplot(Cuyahoga_Delinquents_With_Distance_Info_Geo, aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 100000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```
Looks like most owe less than $100,000 to start. Let's look closer at the distribution of that. 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED<100000) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 5000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```


What is the distribution of Land Bank properties?

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 10000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```
Looks like only a few over $210,000. 



    672-25-001
    EAST CLEVELAND LAND REUTILIZATION PROGRAM
    14600 EUCLID AVE
    EAST CLEVELAND, OH. 44112

An apartment building that was taken over. 



    126-21-014
    THE ZONE, LLC.
    2800 E 90 AVE
    CLEVELAND, OH. 44104


Yup looks like a warehouse that didn't pay taxes for years.Near a lot of construction on 90th.


Primary Owner
CITY OF CLEVELAND HEOIGHTS LAND REUTILIZATION PROGRAM
Property Address
1912 So Taylor RD Cleveland Hts,OH 44118
Tax Mailing Address
CITY OF CLEVELAND HEOIGHTS LAND REUTILIZATION 1900 S TAYLOR RD CLEVELAND, OH 44121
Legal Description
16 MONROE &S/L2 EP & 3 EP BUT TRI 0001 EP
Property Class
GENERAL RETAIL WITH WALK-UP APARTMENTS
Parcel Number
684-26-012
Taxset  
Cleveland Hts.

Yup apartments that didn't pay taxes for years. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  filter(GRAND_TOTAL_OWED<100000) %>% 
ggplot(aes(x=GRAND_TOTAL_OWED))+
  geom_histogram()+
  stat_bin(binwidth = 10000, geom="text",aes(label=..count..), position = position_stack(vjust = .5))
  
```

Of all foreclosures, how much was owed?



```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
what about the percent owed?

```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```


Okay step one: choosing an appropriate model. We want to predict here whether a property will end up in the land bank or not. So we are looking to build a Classification model. 

So first, we have very few Land bank properties versus delinquent properties overall. And this, of course, is our whole story. Who has the power to decide where this will happen? But we will have to take this into account and try to split our training and test sets with an equal number of land bank properties. 

Next, we are going to split the dataset into training and testing models, evenly diving the sections between the Land Bank and non-land bank properties. 

```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
  #filter(Certified_Total_Value>21900&Certified_Total_Value<105100) %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  select(Gov_Foreclosure, is_school_district_majority_white, number_of_high_value_properties_within_500m, owed_as_percentage_of_lot_value) 
    #select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, -Sum_of_distance_to_high_value_properties,  -GRAND_TOTAL_OWED, -GRAND_TOTAL_PAID,  -Certified_Total_Value,  -Close_to_high_value_properties, -number_of_high_value_properties_within_500m, owed_as_percentage_of_lot_value
           

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```

How much do properties owe? 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(Gov_Foreclosure==1) %>% summarize(mean(GRAND_TOTAL_OWED))
 
 Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(Gov_Foreclosure==1) %>% summarize(median(GRAND_TOTAL_OWED))
  
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(Gov_Foreclosure==1) %>% summarize(median(owed_as_percentage_of_lot_value))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(Gov_Foreclosure==1) %>% summarize(mean(owed_as_percentage_of_lot_value))

  Cuyahoga_Delinquents_With_Distance_Info_Geo %>% summarize(mean(owed_as_percentage_of_lot_value))
  
  Cuyahoga_Delinquents_With_Distance_Info_Geo %>% summarize(median(owed_as_percentage_of_lot_value))
 
   Cuyahoga_Delinquents_With_Distance_Info_Geo %>% count(Gov_Foreclosure)

 Cuyahoga_Delinquents_With_Distance_Info_Geo %>% summarize(mean(GRAND_TOTAL_OWED))
 
 
Cuyahoga_Delinquents_With_Distance_Info %>%  #different because trailers don't have geographic info
filter(CLASSIFICATION_DESCR=="House trailer") %>% count()

Cuyahoga_Delinquents_With_Distance_Info %>%  #different because trailers don't have geographic info
filter(CLASSIFICATION_DESCR=="House trailer") %>% summarize(median(owed_as_percentage_of_lot_value))

Cuyahoga_Delinquents_With_Distance_Info %>%  #different because trailers don't have geographic info
filter(CLASSIFICATION_DESCR=="House trailer") %>% summarize(mean(owed_as_percentage_of_lot_value))

Cuyahoga_Delinquents_With_Distance_Info %>%  #different because trailers don't have geographic info
filter(CLASSIFICATION_DESCR=="House trailer") %>% summarize(mean(GRAND_TOTAL_OWED))
 
 
Cuyahoga_Delinquents_With_Distance_Info %>%  #different because trailers don't have geographic info
filter(CLASSIFICATION_DESCR=="House trailer") %>% summarize(median(GRAND_TOTAL_OWED))
 
```









Of the parcels taken over by the land bank, how many are in majority white districts? How does that compare the number of delinquent taxpayers who are in majority white districts?
```{r}
prop.table(table(Cuyahoga_Delinquents_With_Distance_Info_Geo$is_school_district_majority_white, Cuyahoga_Delinquents_With_Distance_Info_Geo$Gov_Foreclosure))
```
On the left, in the column, 0 represents the school district is not majority white, and 1 represents the school district is. 

On top, 0 represents that the property is not a Land bank (or property forfeited to the government.) And 1 is yes.

So 75% of properties were not foreclosed upon and were in minority districts. About 22% of properties were in white district and were not foreclosed upon. Less than a thousandth of all properties were both in a white school district and were foreclosed upon. Three percent of all properties were foreclosed upon and were in a minority district. 


What percentage of delinquent properties are in majority white school districts? What percentage of properties that go to the land bank are in majority white school districts?
```{r}

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% group_by(Gov_Foreclosure) %>% count(is_school_district_majority_white)
```

In majority white school districts, there were 19,515 properties (19443+72). 0.00368947, or 72 out of the 19515 went to white districts. In minority districts, there were 69,967 properties.(66906+2791). 2,791 of those went to the land bank, or 4 percent of those properties. 

But that doesn't take into account how much those properties owed. 

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(mean(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(median(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(mean(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(median(owed_as_percentage_of_lot_value))



Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(mean(Certified_Total_Value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(median(Certified_Total_Value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(mean(Certified_Total_Value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(median(Certified_Total_Value))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(mean(GRAND_TOTAL_OWED))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% summarize(median(GRAND_TOTAL_OWED))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(mean(GRAND_TOTAL_OWED))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% summarize(median(GRAND_TOTAL_OWED))
```
In districts where the majority of students were white, delinquent owners owed an average of 6.6% of their lot value. The median was 1.5%. In minority districts, delinquent owners were behind on average for 64.7% of their lot value, with a median of 4.7%. 

In terms of absolute numbers, delinquent proeprties in majority white districts were valued at an average $278,615, with a median of $154,900. In minority districts, the median was $37,500 and the average was $92,419.

Homes in white districts owed an average of $10,122 and a median of $5,041. In minority districts they owed an average of $9,040 and a median $2,960.


```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% filter(Gov_Foreclosure==1) %>% summarize(median(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(GRAND_TOTAL_OWED))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==1) %>% filter(Gov_Foreclosure==1) %>% summarize(median(GRAND_TOTAL_OWED))




Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(owed_as_percentage_of_lot_value))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(GRAND_TOTAL_OWED))
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% filter(is_school_district_majority_white==0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(GRAND_TOTAL_OWED))
```
Homes in majority white districts and in the land bank (or foreclosed upon by the government) owed an average of 130% of their lot value, with a median of 49.8% of their lot value. Or $22,974 average ($10,637 median.)

Homes in minority districts and in the land bank (or foreclosed upon by the government) owed an average of 391% of their lot value, (with a median of 92%) or a total value of $24,162 average (median $15,804.)


How many expensive parcels within 500 m does the average/median tax delinquent property have? How many do Land Bank properties have?

Since the greatest factor is the grand total owed, we are going to take the amount of tax delinquent into account. 


What's the median owed as a percentage of the lot value? (Since we can't get how many years someone owed money on a property, this is a good proxy for the amount of time a property stood vacant.


```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```
What's the standard deviation?

```{r}
sd(Cuyahoga_Delinquents_With_Distance_Info_Geo$owed_as_percentage_of_lot_value)
```
Let's look at properies around the mean. This is all properties: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

Then properties that weren't foreclosed on

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

And properties that were: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(owed_as_percentage_of_lot_value>(mean(owed_as_percentage_of_lot_value))) %>%  
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

```{r}
summary(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
What's the standard deviation?

```{r}
sd(Cuyahoga_Delinquents_With_Distance_Info_Geo$GRAND_TOTAL_OWED)
```
Let's look at properies around the mean. This is all properties: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>% 
  summarize(mean(number_of_high_value_properties_within_500m, na.rm=TRUE))
```

Then properties that weren't foreclosed on

```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```

And properties that were: 
```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(GRAND_TOTAL_OWED>(mean(GRAND_TOTAL_OWED, na.rm=TRUE)-sd(GRAND_TOTAL_OWED)&GRAND_TOTAL_OWED<mean(GRAND_TOTAL_OWED, na.rm=TRUE)+sd(GRAND_TOTAL_OWED))) %>%   
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(number_of_high_value_properties_within_500m))
```




```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  summarize(mean(is_school_district_majority_white))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(is_school_district_majority_white))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(is_school_district_majority_white))
```




```{r}
Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  summarize(mean(is_school_district_majority_disadvantaged))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==0) %>% 
  summarize(mean(is_school_district_majority_disadvantaged))

Cuyahoga_Delinquents_With_Distance_Info_Geo %>% 
  filter(Gov_Foreclosure==1) %>% 
  summarize(mean(is_school_district_majority_disadvantaged))
```

###Looking at Race



```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
  filter(owed_as_percentage_of_lot_value>.51) %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  #select(Gov_Foreclosure, owed_as_percentage_of_lot_value, number_of_high_value_properties_within_500m, number_of_high_value_properties_within_1000m, Sum_of_distance_to_high_value_properties) 
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, GRAND_TOTAL_BALANCE, -GRAND_TOTAL_OWED, -GRAND_TOTAL_PAID, -GRAND_TOTAL_BALANCE, -number_of_high_value_properties_within_1000m, -Certified_Total_Value, -is_school_district_majority_disadvantaged, is_school_district_majority_white, number_of_high_value_properties_within_500m, -number_of_high_value_properties_within_1000m, 
           )

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```




```{r}
stack_overflow <- Cuyahoga_Delinquents_With_Distance_Info_Geo %>%
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor)


# Create stack_select dataset
stack_select <- stack_overflow %>%
  #select(Gov_Foreclosure, owed_as_percentage_of_lot_value, number_of_high_value_properties_within_500m, number_of_high_value_properties_within_1000m, Sum_of_distance_to_high_value_properties) 
    select(-PROPERTY_NUMBER, -SCHOOL_DESCR, -PRIMARY_OWNER_NAME, -TAX_YEAR,-UPDATE_DATE, -CLASSIFICATION_ID,-CLASSIFICATION_DESCR, -School_Dest_Standardized, -Land_Bank, -Sum_of_distance_to_high_value_properties, -GRAND_TOTAL_BALANCE, -GRAND_TOTAL_PAID, -GRAND_TOTAL_BALANCE,  -Certified_Total_Value, -is_school_district_majority_disadvantaged, -is_school_district_majority_white, -owed_as_percentage_of_lot_value, -number_of_high_value_properties_within_500m
           )

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```








Let's trace the actual values against the predicted ones. 

##Allen County


To map this data, we used parcel shapefiles from [Allen County's Open Data Portal.](http://gis.allencountyohio.com/GIS/downloads.html) 

```{r}
setwd("~/Code/Blue/Housing_Equity/Allen County Current_Parcels/")

Allen_Geo <- sf::st_read("Current_Parcels.shp")
head(Allen_Geo)


```

Column info [can be found here.](https://gis.allencountyohio.com/arcgis/rest/services/SanitaryMobile/FeatureServer/12)

Now we are going to bring in the Land Bank Parcels for the past few years. 

Through a FOIA: 

```{r}
Allen_County_LB_Properties <- rio::import("Allen County Land Bank Properties 1-1-16 to 7-1-21.xlsx")
Allen_County_LB_Properties_With_Amount_Owed <- rio::import("Allen Land Bank Properties with Delinquency.xlsx")


Allen_County_LB_Properties$Delinquency_Amount <- Allen_County_LB_Properties_With_Amount_Owed$`Delinquency Amount`[match(Allen_County_LB_Properties$PARCEL,Allen_County_LB_Properties_With_Amount_Owed$PARCEL)]
```
Note, we actually have two here: the LB properties which has data on where they go them and how they were disposed, and the list that shows the amount owed. Some didn't owe anything and were just given to the land bank.



When were Land Bank Properties acquired?

```{r}
ggplot(Allen_County_LB_Properties, aes(x=year(ACQUIRE)))+
         geom_bar()
```
So this is interesting: There are a few from 2016 and 2018, a bunch in 2018 and not much after that.



Now bringing in Allen County's delinquent list. 

```{r}
Allen_Delinquent <- rio::import("Allen County 2021 03-12 total dlq taxpayer.xlsX")

Allen_Delinquent_2016 <- rio::import("Allen Prosecutor Certified Delq List - Parcles Eligible for Foreclosure 2016.xlsx")
Allen_Delinquent_2017 <- rio::import("Allen Prosecutor DelqCertList - parcels eligible for foreclosure 2017.xlsx")

Allen_Delinquents_2016_2017 <- rbind(Allen_Delinquent_2016, Allen_Delinquent_2017)
```

This is all the delinquents and when they were certified. 



This is amazing, though, there are tax delinquent properties that go way back, such as [this property labeled as delinquent in 1982.](http://allencountyohpropertytax.com/Tax.aspx?mpropertynumber=47-0610-03-001.000&p=47061003001.000)

What is the oldest delinquency?

```{r}
min(Allen_Delinquent$`CertDlq Year`)
```
What is the distribution like? 


```{r}
ggplot(Allen_Delinquent, aes(x=`CertDlq Year`))+
  geom_bar()
```
Okay so this makes sense: most people aren't that behind on their taxes and just pay it off. 


```{r}
datatable(Allen_Delinquent %>% count(`CertDlq Year`))
```

How many were delinquent before 2016? 

```{r}
nrow(filter(Allen_Delinquent, `CertDlq Year`<2016))
```

Okay so really important to note here, we have the 2021 delinquent list AND it shows the original year that someone was certified delinquent. For example, parcels lableled 2015 didn't pay all of 2014 and were still delinquent by 2021. 

So for our purposes, we are going to take the parcels that were certified delinquent before 2020 and were still delinquent in 2021. (Our dataset just has 2019 and before.) 

Because age can make something more vulnerable to pests, crime, etc, we will calculate the age as well. 

```{r}
  Allen_County_LB_Properties_With_Amount_Owed$Delinquent_Year <- Allen_Delinquents_2016_2017$DateCertified[match(Allen_County_LB_Properties_With_Amount_Owed$`Parcel Number`, Allen_Delinquents_2016_2017$PropertyNumber)]


```


Getting stuff into the right format
```{r}
Allen_data <- Allen_Delinquent %>% 
  mutate(Gov_Foreclosure=if_else(Parcel %in% Allen_County_LB_Properties$` Parcel Number`, 1,0))  %>% 
  #filter(Gov_Foreclosure==0) %>% 
  select(PARCEL, `Total Tax Due`,Gov_Foreclosure,`CertDlq Year`) %>% 
  rename(Delinquency_Amount=`Total Tax Due`, Delinquent_Year=`CertDlq Year`)


#Since this delinquent list is from 2021, the land bank properties aren't in there anymore. So adding those in.
Allen_data2 <- Allen_County_LB_Properties_With_Amount_Owed %>% 
  mutate(Gov_Foreclosure=1) %>% 
  select(`Parcel Number`, `Delinquency Amount`, Gov_Foreclosure, Delinquent_Year) %>% 
  rename(Delinquency_Amount=`Delinquency Amount`,PARCEL=`Parcel Number`)

Allen_data <- rbind(Allen_data, Allen_data2)


```

```{r}
Allen_data %>% summarize(sum(Gov_Foreclosure))
```
Calculating the age since delinquency

```{r}
Allen_data$Year_Aquired <- Allen_County_LB_Properties$ACQUIRE[match(Allen_data$PARCEL,Allen_County_LB_Properties$` Parcel Number`)]

Allen_data <- Allen_data %>% 
 mutate(Year_Aquired_All = ifelse(is.na(Year_Aquired),2021, year(Year_Aquired))) %>% 
  mutate(Years_Delinquent=Year_Aquired_All-Delinquent_Year)
                                  

```


How many years was (or is) each property delinquent?

```{r}
ggplot(Allen_data, aes(x=Years_Delinquent, fill=Gov_Foreclosure))+
  geom_bar()
```
So six properties look like they are negative years delinquent. But essentially they went to the land bank, then later they became delinquent again. 



Let's visualize this. 
```{r}
Allen_LB_Geo <- Allen_Geo %>% 
  filter(PARCEL  %in%  Allen_County_LB_Properties$` Parcel Number`)

Allen_LB_Geo <- st_as_sf(Allen_LB_Geo)

plot(Allen_LB_Geo %>% select(PARCEL, geometry))

```
And with a background map so we can actually read it. 

```{r}
Allen_LB_Geo <- st_transform(Allen_LB_Geo, crs = 4326)
Allen_LB_Geo <- st_zm(Allen_LB_Geo, drop = T, what = "ZM")


```


```{r}
Allen_Properties_plotted<- leaflet(data = Allen_LB_Geo) %>% 
  addTiles() %>% 
  setView( -84.10603998384539, 40.73151049471383, zoom = 10) %>% #Set to Lima Center
  addPolygons(data=Allen_LB_Geo$geometry, options = tileOptions(minZoom = 0, maxZoom = 14, continuousWorld = T), popup = Allen_LB_Geo$PARCEL)
  
Allen_Properties_plotted
```


Where are the high-value properties?

```{r}
Allen_Geo_Millions <- Allen_Geo %>% filter(MKTTOTVAL>10000000)
```

```{r}
Allen_County_Delinquent_Geo <- filter(Allen_Geo, PARCEL %in% Allen_data$PARCEL)
```



How close are they to our properties?

```{r }
library(lwgeom)

sf::sf_use_s2(FALSE)

Allen_Distance_Matrix <- as.data.frame(st_distance(  Allen_Geo_Millions$geometry, Allen_County_Delinquent_Geo$geometry)) #This takes quite a while just FYI



  #Sets column names
  
Allen_Distance_Matrix1 <- Allen_Distance_Matrix %>% 
  `colnames<-`(Allen_County_Delinquent_Geo$PARCEL ) %>% 
  
  #Adds a column containing names so that each row now also has a name
  cbind(name = Allen_Geo_Millions$PARCEL  ) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.

```



Removing the units (which interestingly here are in feet.) 

```{r}
library(units)
Allen_Distance_Matrix_No_Units <- drop_units(Allen_Distance_Matrix1)
```


Finding the number of high-value parcels within half a mile of the delinquent properties. 

```{r}
Allen_Number_Near <- Allen_Distance_Matrix_No_Units %>%                                                                                  
  dplyr::summarise_all(~sum(.<2640, na.rm = TRUE))
```

And for comparison, also the number twice as far. 

```{r}
Allen_Number_Semi_Near <- Allen_Distance_Matrix_No_Units %>%                                                                           
  dplyr::summarise_all(~sum(.<5280, na.rm = TRUE))
```

```{r}
Allen_Distance_Info1 <-Allen_Number_Near %>% 
  select(-name) %>% 
  pivot_longer("25-1907-02-007.000":"48-3300-01-002.000", names_to = "PARCEL", values_to = "number_of_high_value_properties_within_half_mile" )
```

```{r}
Allen_Distance_Info2 <-Allen_Number_Semi_Near %>% 
  select(-name) %>% 
  pivot_longer("25-1907-02-007.000":"48-3300-01-002.000", names_to = "PARCEL", values_to = "number_of_high_value_properties_within_mile" )
```


Adding them together

```{r}
Allen_data <- left_join(Allen_data, Allen_Distance_Info1,  by="PARCEL")
Allen_data <- left_join(Allen_data, Allen_Distance_Info2,  by="PARCEL")
```

Which properties were just near expensive properties, period?
```{r}
Allen_data <- Allen_data %>% 
  mutate(Within_half_mile=if_else(number_of_high_value_properties_within_half_mile>=1,1,0)) %>% 
 mutate(Within_mile=if_else(number_of_high_value_properties_within_mile>=1,1,0))          
```





So we have the school district for each parcel. Though it's just a number between 203 and 207  with the exception of 6901. So we are going to input the school district shapefile and then see wheich district each is in. 

```{r}
setwd("Allen_School_Districts/")


Allen_School_Districts <- sf::st_read("School_Districts.shp")
```


Putting them together


```{r}
Allen_Delinquents_with_schools <- st_join(Allen_School_Districts, Allen_County_Delinquent_Geo)
```
 Looks like a few are in multiple school districts. 
 
```{r}
Allen_Dupllicate_Parcels <- Allen_Delinquents_with_schools %>% group_by(PARCEL_NO) %>% summarize(Total=n()) %>% filter(Total>1) %>% ungroup()

datatable(Allen_Dupllicate_Parcels)
```
 

```{r}
Desciphering_Allen_Codes <- Allen_Delinquents_with_schools %>% 
  group_by(SCHOOL_DIS) %>% 
  count(SCHOOL)

datatable(Desciphering_Allen_Codes)
```
Looks like some districts have an open enrollment policy such as [Allen East](https://www.ae.k12.oh.us/docs/district/2021-22/interdistrict%20oe%20guidelines%2021.22.pdf?id=3326) and {Shawnee local School district in Lima, OH](https://www.limashawnee.com/page/new-students-open-enrollment)

And it looks like most districts in general have open enrollment policies except for those that surround inner-city districts.[Yikes.](https://www.dispatch.com/restricted/?return=https%3A%2F%2Fwww.dispatch.com%2Fstory%2Fnews%2F2021%2F08%2F25%2Fwhat-ohio-schools-allow-open-enrollment-new-bill-works%2F8162389002%2F)


Because of this, we are not going to add school data. 




And now, the machine learning part. 


```{r}
stack_overflow <- Allen_data %>% 
filter(Delinquency_Amount>0) %>% #Some properties given to the land bank
filter(Years_Delinquent>0)


stack_overflow <- stack_overflow %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor) #%>% 
   #mutate(Within_mile = factor(Within_half_mile, levels = c(1,0)))


# Create stack_select dataset
stack_select <- stack_overflow %>%
    select(Gov_Foreclosure, Delinquency_Amount)
      #Delinquency_Amount, 
           

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```

Comparing LB to non-LB parcels

```{r}
Allen_data %>% filter(`Delinquency_Amount`>0) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% summarize(mean(Delinquency_Amount))

Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(Delinquency_Amount))


Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(mean(Delinquency_Amount))

Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(Years_Delinquent))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(Years_Delinquent))


Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(median(Years_Delinquent))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(mean(Years_Delinquent))

```


```{r}
Allen_data %>% filter(`Delinquency_Amount`>0) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% summarize(mean(Delinquency_Amount))

Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(Delinquency_Amount))


Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(median(Delinquency_Amount))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(mean(Delinquency_Amount))

Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(median(Years_Delinquent))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==1) %>% summarize(mean(Years_Delinquent))


Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(median(Years_Delinquent))
Allen_data %>% filter(`Delinquency_Amount`>0) %>% filter(Gov_Foreclosure==0) %>% summarize(mean(Years_Delinquent))

```












###Montgomery County

Okay, first bringing in the lists of delinquent taxpayers. 

Lists of Delinquent taxpayers are available on the [Montgomery County Auditor's website.](http://www.mctreas.org/mctreas/filedownloads_auditor.cfm)

Here are the codes for the [column names.](http://www.mctreas.org/mctreas/data/delq%20file%20layout.pdf)

For whatever reason, they have 2014 delinquents, then it skips to 2018. 

```{r}
Montgomery_Delinquents_to_2014 <- rio::import("Montgomery Delinquent to 5-31-2014 Delq_20140531.csv")
Montgomery_Delinquents_to_2018 <-  rio::import("Montgomery Delinquent to 8-31-2018 Delq_20180831.csv")
Montgomery_Delinquents_to_2021  <- rio::import("Montgomery Delq_20211130.csv")
```
Putting those together

```{r}
Montgomery_Delinquent <- rbind(Montgomery_Delinquents_to_2014, Montgomery_Delinquents_to_2018, Montgomery_Delinquents_to_2021)

```

```{r}
Montgomery_Delinquent <- Montgomery_Delinquent %>% 
  rename(Record_Update_Date=`ASMTWEN  `)

Montgomery_Delinquent$Record_Update_Date <- dmy(Montgomery_Delinquent$Record_Update_Date)
```



How much was owed each year?

```{r}
  ggplot(Montgomery_Delinquent, aes(x=year(Record_Update_Date), y=NETDELQ))+
         geom_col()
```


Looking for duplicates

```{r}
datatable(Montgomery_Delinquent %>% group_by(`PARCELID      `) %>% count())
```

So here, some parcels stay delinquent the whole time. such as A01 00109 0022, which owed $3,590.42 in 2013, $7,043 in 2018, ad $10,218.30 by 2021.So before we look for the unique parcels, we want to order them by date reverse chronologically, to get the max amount owed. 

Update: that was a terrible idea. Land bank properties which were delinquent in 2018 and then went to the land bank had their tax lien wiped. Which made it seem farther on like properties that owed less  were more likely to go to the land bank. So this just changed the name of the column. 

So for our purposes, we're just going to see, of the properties behind on their taxes in 2018, how many eventually went to the land bank?

```{r}
Montgomery_Delinquent <- Montgomery_Delinquent %>% 
  rename(`Parcel Number`=`PARCELID      `) %>% 
  filter(year(Record_Update_Date)==2018)




#%>% 
 #arrange(desc(Record_Update_Date)) %>% 
           #distinct(`Parcel Number`, .keep_all = TRUE)
```

Next, bringing in the map. Available at http://www.mcauditor.org/downloads/gis_downloads.cfm


```{r}
setwd("Montgomery SHAPEFILE_PARCELLINES_ROW_OLDLOT/")
Montgomery_Geo <- sf::st_read("Parcels.shp")

#head(Montgomery_Geo)

```
Setting map setting (because  Projected CRS is Ohio South which is messing us up)

```{r}
Montgomery_Geo <- st_transform(Montgomery_Geo, crs = st_crs(4326))

st_crs(Montgomery_Geo)
```



Now bringing in the land bank properties.

Note: As per the Land Bank, this is what each code stands for: 
Program Name	Description
	
TFAP-	Tax Foreclosure Acquistion Program
Land Banking-	Land Banking Program
NIP- 	Neighborhood Initiative Program - Demo
DIY-	Do It Yourself renovation program
CRP-	Commercial Program
Non-NIP Demo-	Demo - Non NIP
TNI-	Thriving Neighborhood Program
Partner Project- 	Special community Program

```{r}
Montgomery_LB_1 <- read_excel("Montgomery County Land Bank Property listing.xlsx", sheet =1) #This only brings in the first sheet
Montgomery_LB_2 <- read_excel("Montgomery County Land Bank Property listing.xlsx", sheet =2) #This only brings in the second sheet

Montgomery_LB_1 <- Montgomery_LB_1 %>% rename(Date_Recorded_Or_Disposed=`Deed Recorded Date`)
Montgomery_LB_2 <- Montgomery_LB_2 %>% rename(Date_Recorded_Or_Disposed=`Date Disposed`)



Montgomery_LB_2 <- Montgomery_LB_2 %>% rename(program=Program)

Montgomery_LB <- rbind(Montgomery_LB_1, Montgomery_LB_2)

head(Montgomery_LB)
```

When were these properties acquired or disposed?

```{r}
ggplot(Montgomery_LB, aes(x=year(Date_Recorded_Or_Disposed), fill=Status))+
         geom_bar()
```






Let's take a look at where all those properties are. 

```{r}
Montgomery_LB_Geo <- Montgomery_Geo %>% 
  filter(TAXPINNO %in%  Montgomery_LB$`Parcel Number`)

```
Note: 44 properties or 2.8% missing




```{r}
Montgomery_Properties_plotted <- leaflet(data = Montgomery_LB_Geo) %>% 
  addTiles() %>% 
  setView(-84.19014950126065,39.75795086894034, zoom = 10) %>% #Set to Dayton Center
  addPolygons()
  
Montgomery_Properties_plotted
```


 Which of those delinquent 2018 properties eventually went to the land bank? 


```{r}
Montgomery_data <- Montgomery_Delinquent  %>% 
  mutate(Gov_Foreclosure=if_else(`Parcel Number` %in% Montgomery_LB$`Parcel Number`, 1,0))
```


How many and what percent went to the Land Bank?

```{r}
sum(Montgomery_data$Gov_Foreclosure)

nrow(Montgomery_data)

sum(Montgomery_data$Gov_Foreclosure)/(nrow(Montgomery_data))

```


Note: The land bank has 1,548 parcels and the number of certified tax delinquent properties was only 1401. 

```{r}
Montgomery_LB_Not_Delinquent <- filter(Montgomery_LB, !(`Parcel Number` %in%  Montgomery_Delinquent$`Parcel Number`))
```

It looks like these properties fell behind on taxes and went to the land bank in between. For example, parcel R72 12202 0030 which got behind on taxes at th end of 2014, then was behind through 2016, then went to the land bank on April 7, 2017, which wiped the tax lien. 

So it's important to note that we are working with a subset here: of the properties delinquent in 2014 and 2018, and 2021, about 2% were actually foreclosed upon. 


###Bringing in the school district

```{r}
Montgomery_data$School_District <- Montgomery_Geo$V_SCHOOL[match(Montgomery_data$`Parcel Number`, Montgomery_Geo$TAXPINNO)]
  
```



A few things about this data: 
*They write "<10" for small categories less than 10, instead of an actual number. 
*Unlike on the website, this has decimals. How do you have part of a kid? Clarification from the Ohio Dept of education's Mandy Minick: "we count students as full time equivalents (FTEs) based on the length of time they are enrolled.  A student enrolled all year counts as 1.0 FTE in the total.  A student enrolled for half a year is 0.5 FTEs and so on.  We sum all the FTEs across the entire year (hence, the decimals). The spreadsheet on the left shows the decimals (33.188847), while the one on the right is rounded to the closest whole number for ease of reading."

Note: some properties ended up being in Montgomery County but in some Greene County School districts. 

```{r}

#Montgomery_schools_demographic <- read_xlsx("Montgomery_School_Districts/2021 school districts Montgomery county.xlsx", skip=2)

Montgomery_schools_demographic <- read_xlsx("All School districts Demographic info.xlsx", skip = 2) 

Montgomery_schools_demographic  <- Montgomery_schools_demographic %>% 
  filter(str_detect(District, "Montgomery")|str_detect(District,"Fairborn")|str_detect(District,"Beavercreek")|str_detect(District,"Springboro")|str_detect(District,"Carlisle")|str_detect(District,"Preble Shawnee"))


head(Montgomery_schools_demographic)
```


How many kids are in each district?

We are going to have to _estimate_ here, because if there's less than 10, they don't report the number.

```{r}
Montgomery_schools_demographic_no_remainders <- Montgomery_schools_demographic %>% 
  filter(CountOfEnrolledStudents!="<10")

  
Montgomery_schools_demographic_no_remainders$CountOfEnrolledStudents <- as.double(Montgomery_schools_demographic_no_remainders$CountOfEnrolledStudents)

Montgomery_school_pop <- Montgomery_schools_demographic_no_remainders%>% 
  group_by(District) %>% 
    summarize(Total=sum(CountOfEnrolledStudents))

datatable(Montgomery_school_pop)
```

Here we are going to treat race and socioeconomic status as factors, even though we have numbers. Why? 

These variables are going to affect our data in a categorical way. You either are discriminating on race or class or you are not. The amount of discrimination does not depend on the amount of people. For example, in the [Kaggle Titantic competition](https://www.kaggle.com/c/titanic) (which is great way to learn machine learning!) The passengers' ticket price is listed. Which passengers were more likely to survive?

You are more likely to survive _if you have a first class ticket_. Period. If your first class ticket costs twice as much as the next first class ticket, that didn't make you twice as likely to survive as someone with a ticket half that price. 

Which school districts have large minority populations? Which have populations with a high number of kids in poverty? 


Finding percentage of minority by District: 
```{r}
Montgomery_schools_by_race <- Montgomery_schools_demographic_no_remainders %>% 
  group_by(District,`Race/Ethnicty`,) %>% #typo in original for 'ethnicity'
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Montgomery_schools_by_race)
```

```{r}
Montgomery_schools_by_race %>% 
  filter(`Race/Ethnicty`=="White, Non-Hispanic") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```




Finding percent disadvantaged by district: 
 
```{r}
Montgomery_schools_by_poverty <- Montgomery_schools_demographic_no_remainders %>% 
  group_by(District,Subgroup,) %>% 
  summarize(Total=sum(CountOfEnrolledStudents)) %>% 
  ungroup() %>% 
  group_by(District) %>% 
  mutate(Percent=Total/sum(Total))
            
datatable(Montgomery_schools_by_poverty)
```

```{r}
Montgomery_schools_by_poverty %>% 
  filter(Subgroup=="Economic Disadvantaged") %>% 
  ggplot(aes(x=reorder(District, -Percent), y=Percent))+
           geom_col()+
  theme(axis.text.x = element_text(angle = 45, vjust=1.1, hjust = 1.1, size = 11))
```


And categorizing them: 

```{r}
Montgomery_schools_by_race_with_majority_white <- Montgomery_schools_by_race %>% 
  group_by(District) %>% 
summarize(Majority_White=if_else(`Race/Ethnicty`=="White, Non-Hispanic"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_White_District=if_else(sum(Majority_White)==1,1,0))
  

Montgomery_schools_with_majority_disadvantaged <- Montgomery_schools_by_poverty %>% 
  group_by(District) %>% 
summarize(Majority_Disadvantaged=if_else(Subgroup=="Economic Disadvantaged"&Percent>0.5,1,0)) %>% 
  group_by(District) %>% 
  summarize(Majority_Disadvantaged_District=if_else(sum(Majority_Disadvantaged)==1,1,0))

  
```

How are the school districts represented in our delinquent dataset?

```{r}
Montgomery_Delinquents_by_School_District <- Montgomery_data %>% group_by(`School_District`) %>% summarize(Total=n())
datatable(Montgomery_Delinquents_by_School_District)

```



The names of school districts is the same but listed a bit differently in both data sets, so we have to do a bit of manual matching here. 

```{r}
Montgomery_data <- Montgomery_data %>% 
  mutate(School_Dest_Standardized= case_when(
     School_District=="DAY" ~ "Dayton City - 043844 (Montgomery)" , 
      School_District== "HUB"  ~  "Huber Heights City - 048751 (Montgomery)",
     School_District=="NLB" ~ "New Lebanon Local School District - 048710 (Montgomery)"  ,
     School_District=="TM" ~  "Trotwood-Madison City - 048694 (Montgomery)"  ,
     School_District=="WCR" ~   "West Carrollton City - 045054 (Montgomery)",
     School_District=="KT"   ~  "Kettering City School District - 044180 (Montgomery)",
     School_District=="CV"    ~ "Centerville City - 043737 (Montgomery)",
     School_District=="MDR"    ~ "Mad River Local - 048702 (Montgomery)",
    School_District== "VB"     ~ "Vandalia-Butler City - 044958 (Montgomery)",
     School_District=="JEFTWP" ~ "Jefferson Township Local - 048686 (Montgomery)",
     School_District=="VV"     ~ "Valley View Local - 048744 (Montgomery)",
     School_District=="NMT"    ~ "Northmont City - 048728 (Montgomery)",
     School_District=="MBG"    ~ "Miamisburg City - 044396 (Montgomery)" ,
     School_District=="NRG"    ~ "Northridge Local - 048736 (Montgomery)",
     School_District=="OAK"    ~ "Oakwood City - 044586 (Montgomery)", 
     School_District=="BRK"    ~ "Brookville Local - 048678 (Montgomery)",
    School_District== "NA"   ~ "NA",    
     School_District=="TCN"    ~ "Trotwood-Madison City - 048694 (Montgomery)", 
     School_District=="FBN"    ~ "Fairborn City - 043968 (Greene)" ,#Some are in Montgomery county BUT in Greene County School districts
    School_District== "BVR"~    "Beavercreek City - 047241 (Greene)",
     School_District=="SPBG"~ "Springboro Community City - 050427 (Warren)",
     School_District=="CAR"  ~  "Carlisle Local - 050419 (Warren)",
     School_District=="PS"    ~ "Preble Shawnee Local - 049288 (Preble)",
     School_District=="X" ~ "NA"))


```

And here we incorporate race and socioeconomic status as factors. 

```{r}
Montgomery_data$is_school_district_majority_white <- Montgomery_schools_by_race_with_majority_white$Majority_White_District[match(Montgomery_data$School_Dest_Standardized,Montgomery_schools_by_race_with_majority_white$District)]

Montgomery_data$is_school_district_majority_disadvantaged <- Montgomery_schools_with_majority_disadvantaged$Majority_Disadvantaged_District[match(Montgomery_data$School_Dest_Standardized,Montgomery_schools_with_majority_disadvantaged$District)]


```

Adding in locations

```{r}
Montgomery_data$geometry <- Montgomery_Geo$geometry[match(Montgomery_data$`Parcel Number`,Montgomery_Geo$TAXPINNO)]
```


Next, we want to see if the location near valuable property is a factor. 


Where are the high-value properties?

```{r}
Montgomery_Values <- foreign::read.dbf("cama.dbf")

#The actual value column has commas in it which really messes things up
Montgomery_Values$APPRTOTAL <- 
rmiscutils ::number_with_commas(Montgomery_Values$APPRTOTAL)

Montgomery_Values$APPRTOTAL <- as.double(Montgomery_Values$APPRTOTAL)

Montgomery_Values_Millions <- filter(Montgomery_Values, APPRTOTAL>10000000)

Montgomery_Geo_Millions <- Montgomery_Geo %>% 
  filter(TAXPINNO  %in% Montgomery_Values_Millions$PARID)
```


How close are they to our properties?

```{r }
library(lwgeom)

sf::sf_use_s2(FALSE)

Montgomery_Distance_Matrix <- as.data.frame(st_distance(  Montgomery_Geo_Millions$geometry, Montgomery_data$geometry,)) #This takes quite a while just FYI



  #Sets column names
  
Montgomery_Distance_Matrix1 <- Montgomery_Distance_Matrix %>% 
  `colnames<-`(Montgomery_data$`Parcel Number` ) %>% 
  
  #Adds a column containing names so that each row now also has a name
  cbind(name = Montgomery_Geo_Millions$TAXPINNO  ) 
#Cuyahoga_Distance_Matrix1 is the same as Cuyahoga_Distance_Matrix but with the parcel number down the side  and column names on top so we don't get confused. But obviously that is just a label and not actually part of the calculations (aka a bad data practice) which is why we keep Cuyahoga Distance Matrix as well.

```



Removing the units 

```{r}
library(units)
Montgomery_Distance_Matrix_No_Units <- drop_units(Montgomery_Distance_Matrix1)
```


Finding the number of high-value parcels within half a mile of the delinquent properties. 

```{r}
Montgomery_Number_Near <- Montgomery_Distance_Matrix_No_Units %>%                                                                                  
  dplyr::summarise_all(~sum(.<2640, na.rm = TRUE))
```

And for comparison, also the number twice as far. 

```{r}
Montgomery_Number_Semi_Near <- Montgomery_Distance_Matrix_No_Units %>%                                                                           
  dplyr::summarise_all(~sum(.<5280, na.rm = TRUE))
```

```{r}
Montgomery_Distance_Info1 <-Montgomery_Number_Near %>% 
  select(-name) %>% 
  pivot_longer("A01 00101 0061":"H33300816 0011", names_to = "Parcel Number", values_to = "number_of_high_value_properties_within_half_mile" )
```

```{r}
Montgomery_Distance_Info2 <-Montgomery_Number_Semi_Near %>% 
  select(-name) %>% 
  pivot_longer("A01 00101 0061":"H33300816 0011", names_to = "Parcel Number", values_to = "number_of_high_value_properties_within_mile" )
```


Adding them together

```{r}
Montgomery_data <- left_join(Montgomery_data, Montgomery_Distance_Info1, by="Parcel Number")
Montgomery_data <- left_join(Montgomery_data, Montgomery_Distance_Info2, by="Parcel Number")
```

Which properties were just near expensive properties, period?
```{r}
Montgomery_data  <- Montgomery_data %>% 
  mutate(Within_half_mile=if_else(number_of_high_value_properties_within_half_mile>=1,1,0)) %>% 
 mutate(Within_mile=if_else(number_of_high_value_properties_within_mile>=1,1,0))          
```



To ask the auditor: Why do 573 of these 34,626 properties have a net delinquency of zero? (Two of which go to the Land bank?)
```{r}
Montgomery_data <- Montgomery_data %>% 
  filter(NETDELQ>0)
```

Also 329 have a property value of 0, which doesn't make sense. Will ask that as well.
```{r}
Montgomery_data <- Montgomery_data %>% 
  filter(ASMTTOTAL>0)
```

Converting percent owed
```{r}
Montgomery_data <- Montgomery_data %>% 
  mutate(owed_as_a_percentage_of_lot_value=NETDELQ/ASMTTOTAL)
```

And now for the machine learning part



```{r}
stack_overflow <- as.data.frame(Montgomery_data)


stack_overflow <- stack_overflow %>% 
    mutate(Gov_Foreclosure = factor(Gov_Foreclosure, levels = c(1,0))) %>%
    mutate_if(is.character, factor) #%>% 
   #mutate(Within_mile = factor(Within_half_mile, levels = c(1,0)))


# Create stack_select dataset
stack_select <- stack_overflow %>%
    select("Gov_Foreclosure",
           NETDELQ,
      #owed_as_a_percentage_of_lot_value,                                                        
      #"is_school_district_majority_white",
          #"is_school_district_majority_disadvantaged",
           #"number_of_high_value_properties_within_half_mile", #not stastically significant 
      "number_of_high_value_properties_within_mile",   
           #"Within_half_mile",   
           #"Within_mile"       
    )

#stack_select$is_school_district_majority_white <- as.factor(stack_select$is_school_district_majority_white)

# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
    initial_split(prop = 0.8,
                  strata = Gov_Foreclosure)

stack_train <- training(stack_split)
stack_test <- testing(stack_split)

glimpse(stack_train)
glimpse(stack_test)
```

Now, we get to the class imbalance in our data set. We don't want our machine learning model to always predict the majority class. 


Here, we're going to use downsampling, (AKA undersampling)- we randomly remove observations from the majority class until it's the same size as the minority class and both classes can have the same effect on the machine learning model we're training. 


```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

stack_prep <- prep(stack_recipe)

stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
    count(Gov_Foreclosure)
```

Note: we are *only* downsampling the training data, not the test data, because we want the test data to be like how this would happen in real life. 

First, we're going to build a logistical regression model

```{r}
stack_recipe <- recipe(Gov_Foreclosure~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

glm_spec <- logistic_reg() %>%
    set_engine("glm")

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_glm <- stack_wf %>%
    add_model(glm_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_glm
```


Next, we're going to build a decision tree model. 

```{r}
stack_recipe <- recipe(Gov_Foreclosure ~ ., data = stack_train) %>% 
    step_downsample(Gov_Foreclosure)

tree_spec <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
stack_wf <- workflow() %>%
    add_recipe(stack_recipe)

## Add the model and fit the workflow
stack_tree <- stack_wf %>%
    add_model(tree_spec) %>%
    fit(data = stack_train)

# Print the fitted model
stack_tree
```

Now we're going to see how well our models performed.  A confusion matrix tabulates how many examples in each class were correctly classified by a model. 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_glm)
```


Confusion Maxtrix for decision tree model

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = Gov_Foreclosure, estimate = .pred_tree)
```

Now let's calculate the accuracy of our results: 

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_glm, stack_test) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

## Calculate accuracy
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_glm)
accuracy(results, truth = Gov_Foreclosure, estimate = .pred_tree)

## Calculate positive predict value
ppv(results, truth = Gov_Foreclosure, estimate = .pred_glm)
ppv(results, truth = Gov_Foreclosure, estimate = .pred_tree)
```

Which variables are particularly important? 


```{r}
vip_tree <- extract_fit_parsnip(stack_tree)$fit
vip_tree$variable.importance
```
Looking at the importance in the logistic regression: 
```{r}
datatable(tidy(stack_glm))

```

Montgomery parcels

```{r}
Montgomery_data %>% filter(Gov_Foreclosure==1) %>% nrow()

Montgomery_data %>%  top_n(278, owed_as_a_percentage_of_lot_value) %>% summarize(sum(Gov_Foreclosure))

Montgomery_data %>% top_n(278, NETDELQ) %>% summarize(sum(Gov_Foreclosure))

```

How much did land bank parcels owe?

```{r}
Montgomery_data %>% filter(Gov_Foreclosure==1) %>% summarize(mean(NETDELQ))
Montgomery_data %>% filter(Gov_Foreclosure==1) %>% summarize(median(NETDELQ))

Montgomery_data %>% top_n(278, NETDELQ) %>% summarize(mean(NETDELQ))
Montgomery_data %>% top_n(278, NETDELQ) %>% summarize(median(NETDELQ))
```

How many were still delinquent three years later?
```{r}
Montgomery_Worst_Offenders <- Montgomery_data %>% top_n(278, NETDELQ) %>% filter(`Parcel Number` %in% Montgomery_Delinquents_to_2021$`PARCELID      `) 

nrow(Montgomery_Worst_Offenders)/278
```
